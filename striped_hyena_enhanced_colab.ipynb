{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thePegasusai/stripedhyena/blob/main/striped_hyena_enhanced_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "introduction"
      },
      "source": [
        "# StripedHyena Neural Style Transfer - Enhanced TPU Version\n",
        "\n",
        "This notebook provides an enhanced implementation of neural style transfer using the StripedHyena architecture developed by Liquid AI. The implementation is specifically optimized for Google Colab's TPU environment.\n",
        "\n",
        "## What's New in This Enhanced Version\n",
        "\n",
        "- **Detailed Architecture Explanations**: In-depth explanations of the StripedHyena architecture components\n",
        "- **Interactive Examples**: User-friendly interface for experimenting with different content and style images\n",
        "- **Custom Dataset Training**: Support for training on your own dataset of images\n",
        "- **TPU Optimizations**: Performance enhancements specifically for Google Colab's TPU environment\n",
        "- **Visualization Tools**: Better visualization of the style transfer process and results\n",
        "- **Hyperparameter Tuning**: Interactive controls for adjusting style transfer parameters\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. [Setup and Dependencies](#setup)\n",
        "2. [StripedHyena Architecture Overview](#architecture)\n",
        "3. [Model Implementation](#implementation)\n",
        "4. [Basic Style Transfer Demo](#basic_demo)\n",
        "5. [Advanced Style Transfer with Parameter Tuning](#advanced_demo)\n",
        "6. [Training on Custom Datasets](#custom_training)\n",
        "7. [Performance Analysis: TPU vs. CPU vs. GPU](#performance)\n",
        "8. [Exporting Your Model](#export)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## 1. Setup and Dependencies <a name=\"setup\"></a>\n",
        "\n",
        "First, let's set up our environment and install the necessary dependencies. This notebook is designed to work with TPUs, but will fall back to GPU or CPU if TPUs are not available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "check_tpu"
      },
      "outputs": [],
      "source": [
        "# Check if TPU is available\n",
        "import os\n",
        "import sys\n",
        "\n",
        "if 'COLAB_TPU_ADDR' in os.environ:\n",
        "    IS_TPU_AVAILABLE = True\n",
        "    print(\"TPU is available!\")\n",
        "else:\n",
        "    IS_TPU_AVAILABLE = False\n",
        "    print(\"TPU is not available. Will use GPU or CPU instead.\")\n",
        "\n",
        "    # Check if GPU is available\n",
        "    import torch\n",
        "    if torch.cuda.is_available():\n",
        "        print(f\"GPU is available: {torch.cuda.get_device_name(0)}\")\n",
        "    else:\n",
        "        print(\"GPU is not available. Will use CPU instead.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "install_dependencies"
      },
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "!pip install torch torchvision matplotlib numpy pillow tqdm requests\n",
        "\n",
        "# If TPU is available, install PyTorch XLA\n",
        "if IS_TPU_AVAILABLE:\n",
        "    !pip install cloud-tpu-client==0.10 torch_xla[tpu]==2.0 -f https://storage.googleapis.com/libtpu-releases/index.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup_device"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import time\n",
        "\n",
        "# Set up device\n",
        "if IS_TPU_AVAILABLE:\n",
        "    import torch_xla.core.xla_model as xm\n",
        "    device = xm.xla_device()\n",
        "    print(f\"Using TPU: {device}\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Using CPU\")\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "architecture"
      },
      "source": [
        "## 2. StripedHyena Architecture Overview <a name=\"architecture\"></a>\n",
        "\n",
        "The StripedHyena architecture, developed by Liquid AI, is a hybrid neural network architecture that combines rotary (grouped) attention with gated convolutions. This section provides a detailed explanation of the architecture and its components.\n",
        "\n",
        "### 2.1 Key Components of StripedHyena\n",
        "\n",
        "#### Rotary Positional Embeddings\n",
        "Rotary positional embeddings (RoPE) encode spatial information, allowing the model to understand the relative positions of features in both content and style images. This is crucial for maintaining spatial coherence in the stylized output.\n",
        "\n",
        "```\n",
        "def apply_rotary_embeddings(x, freqs):\n",
        "    # x: [batch, seq_len, dim]\n",
        "    # freqs: [seq_len, dim/2]\n",
        "    seq_len = x.shape[1]\n",
        "    x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
        "    freqs = freqs[:seq_len]\n",
        "    freqs = torch.view_as_complex(freqs.reshape(*freqs.shape[:-1], -1, 2))\n",
        "    x_rotated = x_complex * freqs\n",
        "    x_rotated = torch.view_as_real(x_rotated).flatten(-2)\n",
        "    return x_rotated.type_as(x)\n",
        "```\n",
        "\n",
        "#### Gated Convolutions\n",
        "The architecture employs gated convolutions that adaptively control information flow, allowing the model to selectively apply style features based on content characteristics. This results in more natural-looking style transfers that preserve important content details.\n",
        "\n",
        "```\n",
        "class GatedConv2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels*2, kernel_size, stride, padding)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Split channels into two parts\n",
        "        h = self.conv(x)\n",
        "        a, b = torch.chunk(h, 2, dim=1)\n",
        "        # Apply gating mechanism\n",
        "        return a * torch.sigmoid(b)\n",
        "```\n",
        "\n",
        "#### Hybrid Attention Mechanism\n",
        "StripedHyena combines local processing (through convolutions) with global context awareness (through attention mechanisms), creating a hybrid approach that captures both fine details and overall style patterns. This hybrid design is particularly effective for style transfer tasks.\n",
        "\n",
        "### 2.2 Advantages for Neural Style Transfer\n",
        "\n",
        "1. **Memory Efficiency**: Processes high-resolution images with significantly lower memory requirements than traditional Transformer models.\n",
        "2. **Linear Scaling**: Computational complexity scales linearly with image size instead of quadratically.\n",
        "3. **Long-range Dependencies**: Efficiently captures relationships between distant parts of the image.\n",
        "4. **Adaptive Processing**: Intelligently applies style based on content characteristics.\n",
        "5. **TPU Compatibility**: Architecture is well-suited for TPU acceleration.\n",
        "\n",
        "### 2.3 Adaptation for 2D Images\n",
        "\n",
        "The original StripedHyena architecture was designed for 1D sequence processing. For neural style transfer, we adapt it to 2D image processing by:\n",
        "\n",
        "1. Converting 2D images to sequences by flattening spatial dimensions\n",
        "2. Applying StripedHyena processing\n",
        "3. Reshaping back to 2D for convolutional processing\n",
        "4. Using skip connections to preserve spatial information\n",
        "\n",
        "This approach allows us to leverage the strengths of the StripedHyena architecture while maintaining the spatial structure necessary for image processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "implementation"
      },
      "source": [
        "## 3. Model Implementation <a name=\"implementation\"></a>\n",
        "\n",
        "Now, let's implement the StripedHyena neural style transfer model. We'll start with the core components and then build the complete model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rotary_embeddings"
      },
      "outputs": [],
      "source": [
        "# Rotary Positional Embeddings implementation\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class RotaryEmbedding(nn.Module):\n",
        "    def __init__(self, dim, base=10000):\n",
        "        super().__init__()\n",
        "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
        "        self.register_buffer(\"inv_freq\", inv_freq)\n",
        "        self.seq_len_cached = None\n",
        "        self.cos_cached = None\n",
        "        self.sin_cached = None\n",
        "\n",
        "    def forward(self, x, seq_len=None):\n",
        "        if seq_len is None:\n",
        "            seq_len = x.shape[1]\n",
        "\n",
        "        if seq_len != self.seq_len_cached:\n",
        "            self.seq_len_cached = seq_len\n",
        "            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n",
        "            freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
        "            emb = torch.cat((freqs, freqs), dim=-1).to(x.device)\n",
        "            self.cos_cached = emb.cos()[None, :, None, :]\n",
        "            self.sin_cached = emb.sin()[None, :, None, :]\n",
        "\n",
        "        return self.cos_cached, self.sin_cached\n",
        "\n",
        "# Function to apply rotary embeddings\n",
        "def apply_rotary_pos_emb(q, k, cos, sin):\n",
        "    # Reshape q and k for the rotation\n",
        "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
        "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
        "    return q_embed, k_embed\n",
        "\n",
        "def rotate_half(x):\n",
        "    x1, x2 = x.chunk(2, dim=-1)\n",
        "    return torch.cat((-x2, x1), dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gated_convolutions"
      },
      "outputs": [],
      "source": [
        "# Gated Convolution implementation\n",
        "class GatedConv2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels*2, kernel_size, stride, padding)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.conv(x)\n",
        "        a, b = torch.chunk(h, 2, dim=1)\n",
        "        return a * torch.sigmoid(b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyena_block"
      },
      "outputs": [],
      "source": [
        "# StripedHyena Block implementation\n",
        "class StripedHyenaBlock(nn.Module):\n",
        "    def __init__(self, dim, heads=4, dim_head=64, dropout=0.0):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head * heads\n",
        "        self.heads = heads\n",
        "        self.dim_head = dim_head\n",
        "\n",
        "        # Rotary embeddings\n",
        "        self.rotary_emb = RotaryEmbedding(dim_head)\n",
        "\n",
        "        # Projections for Q, K, V\n",
        "        self.to_q = nn.Linear(dim, inner_dim, bias=False)\n",
        "        self.to_k = nn.Linear(dim, inner_dim, bias=False)\n",
        "        self.to_v = nn.Linear(dim, inner_dim, bias=False)\n",
        "        self.to_out = nn.Linear(inner_dim, dim)\n",
        "\n",
        "        # Gated feed-forward network\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(dim, dim * 4),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(dim * 4, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        # Layer norms\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Layer norm 1\n",
        "        normed_x = self.norm1(x)\n",
        "\n",
        "        # Self-attention with rotary embeddings\n",
        "        q = self.to_q(normed_x)\n",
        "        k = self.to_k(normed_x)\n",
        "        v = self.to_v(normed_x)\n",
        "\n",
        "        # Reshape for multi-head attention\n",
        "        q = q.view(q.shape[0], q.shape[1], self.heads, self.dim_head)\n",
        "        k = k.view(k.shape[0], k.shape[1], self.heads, self.dim_head)\n",
        "        v = v.view(v.shape[0], v.shape[1], self.heads, self.dim_head)\n",
        "\n",
        "        # Apply rotary embeddings\n",
        "        cos, sin = self.rotary_emb(q, seq_len=q.shape[1])\n",
        "        q, k = apply_rotary_pos_emb(q, k, cos, sin)\n",
        "\n",
        "        # Reshape for attention computation\n",
        "        q = q.transpose(1, 2)  # [batch, heads, seq_len, dim_head]\n",
        "        k = k.transpose(1, 2)\n",
        "        v = v.transpose(1, 2)\n",
        "\n",
        "        # Compute attention scores\n",
        "        scale = 1.0 / math.sqrt(self.dim_head)\n",
        "        attn = torch.matmul(q, k.transpose(-2, -1)) * scale\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "\n",
        "        # Apply attention to values\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = out.transpose(1, 2).contiguous().view(out.shape[0], -1, self.heads * self.dim_head)\n",
        "        out = self.to_out(out)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        # First residual connection\n",
        "        x = x + out\n",
        "\n",
        "        # Layer norm 2\n",
        "        normed_x = self.norm2(x)\n",
        "\n",
        "        # Feed-forward network\n",
        "        ff_out = self.ff(normed_x)\n",
        "\n",
        "        # Second residual connection\n",
        "        return x + ff_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "style_transfer_model"
      },
      "outputs": [],
      "source": [
        "# Complete StripedHyena Style Transfer Model\n",
        "class StripedHyenaStyleTransfer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 content_layers=['conv4_2'],\n",
        "                 style_layers=['conv1_1', 'conv2_1', 'conv3_1', 'conv4_1', 'conv5_1']):\n",
        "        super().__init__()\n",
        "\n",
        "        self.content_layers = content_layers\n",
        "        self.style_layers = style_layers\n",
        "\n",
        "        # VGG16 for feature extraction (pre-trained)\n",
        "        vgg = torchvision.models.vgg16(pretrained=True).features\n",
        "        self.vgg = nn.ModuleList()\n",
        "        self.vgg_layer_names = []\n",
        "\n",
        "        # Create sequential modules from VGG16\n",
        "        i = 0\n",
        "        for layer in vgg.children():\n",
        "            if isinstance(layer, nn.Conv2d):\n",
        "                i += 1\n",
        "                name = f'conv{i}'\n",
        "            elif isinstance(layer, nn.ReLU):\n",
        "                name = f'relu{i}'\n",
        "                layer = nn.ReLU(inplace=False)  # Use non-inplace ReLU\n",
        "            elif isinstance(layer, nn.MaxPool2d):\n",
        "                name = f'pool{i}'\n",
        "            elif isinstance(layer, nn.BatchNorm2d):\n",
        "                name = f'bn{i}'\n",
        "            else:\n",
        "                raise RuntimeError(f'Unrecognized layer: {layer.__class__.__name__}')\n",
        "\n",
        "            self.vgg.add_module(name, layer)\n",
        "            self.vgg_layer_names.append(name)\n",
        "\n",
        "        # Freeze VGG parameters\n",
        "        for param in self.vgg.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # StripedHyena blocks for content and style processing\n",
        "        self.content_hyena = nn.ModuleList([\n",
        "            StripedHyenaBlock(512, heads=8, dim_head=64, dropout=0.1)\n",
        "            for _ in range(3)\n",
        "        ])\n",
        "\n",
        "        self.style_hyena = nn.ModuleList([\n",
        "            StripedHyenaBlock(512, heads=8, dim_head=64, dropout=0.1)\n",
        "            for _ in range(3)\n",
        "        ])\n",
        "\n",
        "        # Decoder network\n",
        "        self.decoder = nn.Sequential(\n",
        "            GatedConv2d(512, 256, kernel_size=3, padding=1),\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "            GatedConv2d(256, 256, kernel_size=3, padding=1),\n",
        "            GatedConv2d(256, 256, kernel_size=3, padding=1),\n",
        "            GatedConv2d(256, 128, kernel_size=3, padding=1),\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "            GatedConv2d(128, 128, kernel_size=3, padding=1),\n",
        "            GatedConv2d(128, 64, kernel_size=3, padding=1),\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "            GatedConv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.Conv2d(64, 3, kernel_size=3, padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode_features(self, x):\n",
        "        \"\"\"Extract VGG features\"\"\"\n",
        "        features = {}\n",
        "        for name, layer in self.vgg._modules.items():\n",
        "            x = layer(x)\n",
        "            if name in self.content_layers or name in self.style_layers:\n",
        "                features[name] = x\n",
        "        return features\n",
        "\n",
        "    def process_content(self, content_features):\n",
        "        \"\"\"Process content features through StripedHyena blocks\"\"\"\n",
        "        # Use the deepest content feature\n",
        "        x = content_features[self.content_layers[-1]]\n",
        "        batch_size, channels, height, width = x.shape\n",
        "\n",
        "        # Reshape to sequence for StripedHyena processing\n",
        "        x = x.view(batch_size, channels, -1).permute(0, 2, 1)  # [batch, seq_len, channels]\n",
        "\n",
        "        # Apply StripedHyena blocks\n",
        "        for block in self.content_hyena:\n",
        "            x = block(x)\n",
        "\n",
        "        # Reshape back to spatial\n",
        "        x = x.permute(0, 2, 1).view(batch_size, channels, height, width)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def process_style(self, style_features):\n",
        "        \"\"\"Process style features through StripedHyena blocks\"\"\"\n",
        "        # Use the deepest style feature\n",
        "        x = style_features[self.style_layers[-1]]\n",
        "        batch_size, channels, height, width = x.shape\n",
        "\n",
        "        # Reshape to sequence for StripedHyena processing\n",
        "        x = x.view(batch_size, channels, -1).permute(0, 2, 1)  # [batch, seq_len, channels]\n",
        "\n",
        "        # Apply StripedHyena blocks\n",
        "        for block in self.style_hyena:\n",
        "            x = block(x)\n",
        "\n",
        "        # Reshape back to spatial\n",
        "        x = x.permute(0, 2, 1).view(batch_size, channels, height, width)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, content_img, style_img, alpha=1.0, return_features=False):\n",
        "        \"\"\"Forward pass for style transfer\"\"\"\n",
        "        # Extract features\n",
        "        content_features = self.encode_features(content_img)\n",
        "        style_features = self.encode_features(style_img)\n",
        "\n",
        "        # Process content and style features\n",
        "        processed_content = self.process_content(content_features)\n",
        "        processed_style = self.process_style(style_features)\n",
        "\n",
        "        # Combine content and style features\n",
        "        combined = processed_content * alpha + processed_style * (1 - alpha)\n",
        "\n",
        "        # Decode to generate stylized image\n",
        "        output_img = self.decoder(combined)\n",
        "\n",
        "        if return_features:\n",
        "            return output_img, content_features, style_features, {\n",
        "                'content': processed_content,\n",
        "                'style': style_features  # Use original style features for loss computation\n",
        "            }\n",
        "\n",
        "        return output_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "initialize_model"
      },
      "outputs": [],
      "source": [
        "# Initialize the model\n",
        "model = StripedHyenaStyleTransfer().to(device)\n",
        "print(f\"Model initialized on {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "basic_demo"
      },
      "source": [
        "## 4. Basic Style Transfer Demo <a name=\"basic_demo\"></a>\n",
        "\n",
        "Let's create a basic demo to demonstrate the style transfer capabilities of our StripedHyena model. We'll use some sample images and apply the style transfer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "image_utils"
      },
      "outputs": [],
      "source": [
        "# Utility functions for image processing\n",
        "def load_image_from_url(url, max_size=512):\n",
        "    \"\"\"Load an image from a URL\"\"\"\n",
        "    response = requests.get(url)\n",
        "    img = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "\n",
        "    # Resize while maintaining aspect ratio\n",
        "    if max(img.size) > max_size:\n",
        "        ratio = max_size / max(img.size)\n",
        "        new_size = (int(img.size[0] * ratio), int(img.size[1] * ratio))\n",
        "        img = img.resize(new_size, Image.LANCZOS)\n",
        "\n",
        "    return img\n",
        "\n",
        "def load_image_from_upload(uploaded_file, max_size=512):\n",
        "    \"\"\"Load an image from an uploaded file\"\"\"\n",
        "    img = Image.open(uploaded_file).convert('RGB')\n",
        "\n",
        "    # Resize while maintaining aspect ratio\n",
        "    if max(img.size) > max_size:\n",
        "        ratio = max_size / max(img.size)\n",
        "        new_size = (int(img.size[0] * ratio), int(img.size[1] * ratio))\n",
        "        img = img.resize(new_size, Image.LANCZOS)\n",
        "\n",
        "    return img\n",
        "\n",
        "def preprocess_image(img):\n",
        "    \"\"\"Convert PIL image to tensor for model input\"\"\"\n",
        "    transform = torchvision.transforms.Compose([\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    img_tensor = transform(img).unsqueeze(0).to(device)\n",
        "    return img_tensor\n",
        "\n",
        "def deprocess_image(tensor):\n",
        "    \"\"\"Convert tensor to PIL image for display\"\"\"\n",
        "    # Move to CPU if needed\n",
        "    if IS_TPU_AVAILABLE:\n",
        "        tensor = xm.mesh_reduce('tensor_to_cpu', tensor, lambda x: x.cpu())\n",
        "    elif tensor.is_cuda:\n",
        "        tensor = tensor.cpu()\n",
        "\n",
        "    # Denormalize\n",
        "    tensor = tensor.squeeze(0).detach().clone()\n",
        "    tensor = tensor.clamp(0, 1)\n",
        "\n",
        "    # Convert to PIL image\n",
        "    img = torchvision.transforms.ToPILImage()(tensor)\n",
        "    return img\n",
        "\n",
        "def display_images(content_img, style_img, output_img, figsize=(15, 5)):\n",
        "    \"\"\"Display content, style, and output images side by side\"\"\"\n",
        "    fig, axes = plt.subplots(1, 3, figsize=figsize)\n",
        "\n",
        "    axes[0].imshow(content_img)\n",
        "    axes[0].set_title(\"Content Image\")\n",
        "    axes[0].axis(\"off\")\n",
        "\n",
        "    axes[1].imshow(style_img)\n",
        "    axes[1].set_title(\"Style Image\")\n",
        "    axes[1].axis(\"off\")\n",
        "\n",
        "    axes[2].imshow(output_img)\n",
        "    axes[2].set_title(\"Stylized Output\")\n",
        "    axes[2].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sample_images"
      },
      "outputs": [],
      "source": [
        "# Sample images for demonstration\n",
        "content_url = \"https://images.pexels.com/photos/2559941/pexels-photo-2559941.jpeg\"\n",
        "style_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg/1280px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg\"\n",
        "\n",
        "try:\n",
        "    content_img = load_image_from_url(content_url)\n",
        "    style_img = load_image_from_url(style_url)\n",
        "\n",
        "    # Display original images\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "    axes[0].imshow(content_img)\n",
        "    axes[0].set_title(\"Content Image\")\n",
        "    axes[0].axis(\"off\")\n",
        "\n",
        "    axes[1].imshow(style_img)\n",
        "    axes[1].set_title(\"Style Image\")\n",
        "    axes[1].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "except Exception as e:\n",
        "    print(f\"Error loading sample images: {e}\")\n",
        "    print(\"Please upload your own images in the next cell.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "basic_style_transfer"
      },
      "outputs": [],
      "source": [
        "# Basic style transfer\n",
        "def perform_style_transfer(content_img, style_img, alpha=1.0):\n",
        "    \"\"\"Perform style transfer using the StripedHyena model\"\"\"\n",
        "    # Preprocess images\n",
        "    content_tensor = preprocess_image(content_img)\n",
        "    style_tensor = preprocess_image(style_img)\n",
        "\n",
        "    # Perform style transfer\n",
        "    with torch.no_grad():\n",
        "        start_time = time.time()\n",
        "        output_tensor = model(content_tensor, style_tensor, alpha=alpha)\n",
        "\n",
        "        # Handle TPU synchronization if needed\n",
        "        if IS_TPU_AVAILABLE:\n",
        "            xm.mark_step()\n",
        "\n",
        "        end_time = time.time()\n",
        "\n",
        "    # Deprocess output\n",
        "    output_img = deprocess_image(output_tensor)\n",
        "\n",
        "    print(f\"Style transfer completed in {end_time - start_time:.2f} seconds\")\n",
        "    return output_img\n",
        "\n",
        "# Run style transfer on sample images\n",
        "try:\n",
        "    output_img = perform_style_transfer(content_img, style_img, alpha=0.8)\n",
        "    display_images(content_img, style_img, output_img)\n",
        "except NameError:\n",
        "    print(\"Please upload content and style images first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "advanced_demo"
      },
      "source": [
        "## 5. Advanced Style Transfer with Parameter Tuning <a name=\"advanced_demo\"></a>\n",
        "\n",
        "Now, let's create a more advanced demo that allows for parameter tuning and interactive experimentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upload_images"
      },
      "outputs": [],
      "source": [
        "# Upload your own images\n",
        "from google.colab import files\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "def upload_images():\n",
        "    print(\"Please upload a content image:\")\n",
        "    content_file = files.upload()\n",
        "    content_filename = list(content_file.keys())[0]\n",
        "    content_img = load_image_from_upload(content_filename)\n",
        "\n",
        "    print(\"\\nPlease upload a style image:\")\n",
        "    style_file = files.upload()\n",
        "    style_filename = list(style_file.keys())[0]\n",
        "    style_img = load_image_from_upload(style_filename)\n",
        "\n",
        "    return content_img, style_img\n",
        "\n",
        "# Uncomment to upload your own images\n",
        "# custom_content_img, custom_style_img = upload_images()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "interactive_demo"
      },
      "outputs": [],
      "source": [
        "# Interactive demo with parameter tuning\n",
        "def interactive_style_transfer(content_img, style_img):\n",
        "    # Create widgets\n",
        "    alpha_slider = widgets.FloatSlider(\n",
        "        value=0.8,\n",
        "        min=0.0,\n",
        "        max=1.0,\n",
        "        step=0.05,\n",
        "        description='Content Weight:',\n",
        "        continuous_update=False\n",
        "    )\n",
        "\n",
        "    run_button = widgets.Button(description=\"Apply Style Transfer\")\n",
        "    output = widgets.Output()\n",
        "\n",
        "    # Display original images\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "    axes[0].imshow(content_img)\n",
        "    axes[0].set_title(\"Content Image\")\n",
        "    axes[0].axis(\"off\")\n",
        "\n",
        "    axes[1].imshow(style_img)\n",
        "    axes[1].set_title(\"Style Image\")\n",
        "    axes[1].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Define button click handler\n",
        "    def on_button_clicked(b):\n",
        "        with output:\n",
        "            clear_output()\n",
        "            print(f\"Running style transfer with content weight = {alpha_slider.value}...\")\n",
        "            output_img = perform_style_transfer(content_img, style_img, alpha=alpha_slider.value)\n",
        "            display_images(content_img, style_img, output_img)\n",
        "\n",
        "    # Connect button to handler\n",
        "    run_button.on_click(on_button_clicked)\n",
        "\n",
        "    # Display widgets\n",
        "    display(alpha_slider, run_button, output)\n",
        "\n",
        "# Run interactive demo with sample images\n",
        "try:\n",
        "    interactive_style_transfer(content_img, style_img)\n",
        "except NameError:\n",
        "    print(\"Please upload content and style images first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "custom_training"
      },
      "source": [
        "## 6. Training on Custom Datasets <a name=\"custom_training\"></a>\n",
        "\n",
        "In this section, we'll implement the training process for the StripedHyena neural style transfer model using custom datasets. This allows you to train the model on your own collection of content and style images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dataset_loader"
      },
      "outputs": [],
      "source": [
        "# Dataset and DataLoader implementation\n",
        "import os\n",
        "import random\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class StyleTransferDataset(Dataset):\n",
        "    \"\"\"Dataset for neural style transfer training\"\"\"\n",
        "\n",
        "    def __init__(self, content_dir, style_dir, image_size=256, transform=None):\n",
        "        \"\"\"\n",
        "        Initialize the dataset.\n",
        "\n",
        "        Args:\n",
        "            content_dir: Directory containing content images\n",
        "            style_dir: Directory containing style images\n",
        "            image_size: Size to resize images to\n",
        "            transform: Optional transform to apply to images\n",
        "        \"\"\"\n",
        "        self.content_paths = []\n",
        "        self.style_paths = []\n",
        "\n",
        "        # Check if directories exist\n",
        "        if os.path.exists(content_dir):\n",
        "            self.content_paths = [os.path.join(content_dir, f) for f in os.listdir(content_dir)\n",
        "                                if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "        else:\n",
        "            print(f\"Warning: Content directory {content_dir} not found.\")\n",
        "\n",
        "        if os.path.exists(style_dir):\n",
        "            self.style_paths = [os.path.join(style_dir, f) for f in os.listdir(style_dir)\n",
        "                               if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "        else:\n",
        "            print(f\"Warning: Style directory {style_dir} not found.\")\n",
        "\n",
        "        if not self.content_paths:\n",
        "            raise ValueError(f\"No content images found in {content_dir}\")\n",
        "\n",
        "        if not self.style_paths:\n",
        "            raise ValueError(f\"No style images found in {style_dir}\")\n",
        "\n",
        "        self.image_size = image_size\n",
        "\n",
        "        if transform is None:\n",
        "            self.transform = torchvision.transforms.Compose([\n",
        "                torchvision.transforms.Resize(image_size),\n",
        "                torchvision.transforms.CenterCrop(image_size),\n",
        "                torchvision.transforms.ToTensor(),\n",
        "                torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                              std=[0.229, 0.224, 0.225])\n",
        "            ])\n",
        "        else:\n",
        "            self.transform = transform\n",
        "\n",
        "        print(f\"Found {len(self.content_paths)} content images and {len(self.style_paths)} style images\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.content_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load content image\n",
        "        content_path = self.content_paths[idx]\n",
        "        content_img = Image.open(content_path).convert('RGB')\n",
        "        content_tensor = self.transform(content_img)\n",
        "\n",
        "        # Randomly select a style image\n",
        "        style_path = random.choice(self.style_paths)\n",
        "        style_img = Image.open(style_path).convert('RGB')\n",
        "        style_tensor = self.transform(style_img)\n",
        "\n",
        "        return {\n",
        "            'content': content_tensor,\n",
        "            'style': style_tensor,\n",
        "            'content_path': content_path,\n",
        "            'style_path': style_path\n",
        "        }\n",
        "\n",
        "def get_dataloader(content_dir, style_dir, batch_size=4, image_size=256, num_workers=2):\n",
        "    \"\"\"Create and return the data loader\"\"\"\n",
        "    dataset = StyleTransferDataset(content_dir, style_dir, image_size)\n",
        "\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True if torch.cuda.is_available() else False\n",
        "    )\n",
        "\n",
        "    return dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loss_functions"
      },
      "outputs": [],
      "source": [
        "# Loss functions for neural style transfer\n",
        "class ContentLoss(nn.Module):\n",
        "    \"\"\"Content loss for neural style transfer\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ContentLoss, self).__init__()\n",
        "        self.criterion = nn.MSELoss()\n",
        "\n",
        "    def forward(self, output_features, target_features):\n",
        "        return self.criterion(output_features, target_features)\n",
        "\n",
        "class StyleLoss(nn.Module):\n",
        "    \"\"\"Style loss for neural style transfer\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(StyleLoss, self).__init__()\n",
        "        self.criterion = nn.MSELoss()\n",
        "\n",
        "    def forward(self, output_features, target_features):\n",
        "        # Calculate Gram matrices\n",
        "        output_gram = self.gram_matrix(output_features)\n",
        "        target_gram = self.gram_matrix(target_features)\n",
        "        return self.criterion(output_gram, target_gram)\n",
        "\n",
        "    def gram_matrix(self, features):\n",
        "        \"\"\"Calculate Gram matrix for style loss\"\"\"\n",
        "        batch_size, ch, h, w = features.size()\n",
        "        features = features.view(batch_size, ch, h * w)\n",
        "        features_t = features.transpose(1, 2)\n",
        "        gram = features.bmm(features_t) / (ch * h * w)\n",
        "        return gram\n",
        "\n",
        "class TVLoss(nn.Module):\n",
        "    \"\"\"Total variation loss for smoothing\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(TVLoss, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, c, h, w = x.size()\n",
        "        tv_h = torch.pow(x[:, :, 1:, :] - x[:, :, :-1, :], 2).sum()\n",
        "        tv_w = torch.pow(x[:, :, :, 1:] - x[:, :, :, :-1], 2).sum()\n",
        "        return (tv_h + tv_w) / (batch_size * c * h * w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "training_function"
      },
      "outputs": [],
      "source": [
        "# Training function\n",
        "def train_model(content_dir, style_dir,\n",
        "                num_epochs=100,\n",
        "                batch_size=4,\n",
        "                learning_rate=1e-4,\n",
        "                content_weight=1.0,\n",
        "                style_weight=10.0,\n",
        "                tv_weight=0.001,\n",
        "                checkpoint_interval=10,\n",
        "                image_size=256):\n",
        "    \"\"\"Train the StripedHyena neural style transfer model\"\"\"\n",
        "\n",
        "    # Create output directories\n",
        "    os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "    os.makedirs(\"results\", exist_ok=True)\n",
        "\n",
        "    # Initialize model\n",
        "    model = StripedHyenaStyleTransfer().to(device)\n",
        "\n",
        "    # Initialize loss functions\n",
        "    content_criterion = ContentLoss().to(device)\n",
        "    style_criterion = StyleLoss().to(device)\n",
        "    tv_criterion = TVLoss().to(device)\n",
        "\n",
        "    # Initialize optimizer\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Get data loader\n",
        "    dataloader = get_dataloader(content_dir, style_dir, batch_size, image_size)\n",
        "\n",
        "    # Training loop\n",
        "    losses = []\n",
        "\n",
        "    print(f\"Starting training for {num_epochs} epochs...\")\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0.0\n",
        "        start_time = time.time()\n",
        "\n",
        "        model.train()\n",
        "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        for batch in progress_bar:\n",
        "            # Move data to device\n",
        "            content_images = batch['content'].to(device)\n",
        "            style_images = batch['style'].to(device)\n",
        "\n",
        "            # Zero gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            output_images, content_features, style_features, output_features = model(\n",
        "                content_images, style_images, return_features=True\n",
        "            )\n",
        "\n",
        "            # Calculate losses\n",
        "            content_loss = content_criterion(output_features['content'], content_features[model.content_layers[0]])\n",
        "\n",
        "            style_loss = 0\n",
        "            for layer in model.style_layers:\n",
        "                style_loss += style_criterion(output_features['style'][layer], style_features[layer])\n",
        "            style_loss /= len(model.style_layers)\n",
        "\n",
        "            tv_loss = tv_criterion(output_images)\n",
        "\n",
        "            # Total loss\n",
        "            loss = content_weight * content_loss + style_weight * style_loss + tv_weight * tv_loss\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Handle TPU synchronization if needed\n",
        "            if IS_TPU_AVAILABLE:\n",
        "                xm.mark_step()\n",
        "\n",
        "            # Update progress\n",
        "            epoch_loss += loss.item()\n",
        "            progress_bar.set_postfix({\n",
        "                \"loss\": f\"{loss.item():.4f}\",\n",
        "                \"content\": f\"{content_loss.item():.4f}\",\n",
        "                \"style\": f\"{style_loss.item():.4f}\"\n",
        "            })\n",
        "\n",
        "        # Calculate average epoch loss\n",
        "        avg_epoch_loss = epoch_loss / len(dataloader)\n",
        "        losses.append(avg_epoch_loss)\n",
        "\n",
        "        # Print epoch summary\n",
        "        elapsed_time = time.time() - start_time\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_epoch_loss:.4f} - Time: {elapsed_time:.2f}s\")\n",
        "\n",
        "        # Save checkpoint\n",
        "        if (epoch + 1) % checkpoint_interval == 0:\n",
        "            checkpoint_path = f\"checkpoints/model_epoch_{epoch+1}.pth\"\n",
        "            torch.save({\n",
        "                'epoch': epoch + 1,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': avg_epoch_loss,\n",
        "            }, checkpoint_path)\n",
        "            print(f\"Checkpoint saved to {checkpoint_path}\")\n",
        "\n",
        "    # Save final model\n",
        "    torch.save(model.state_dict(), \"checkpoints/model_final.pth\")\n",
        "    print(\"Training complete. Final model saved.\")\n",
        "\n",
        "    # Plot loss curve\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(range(1, num_epochs + 1), losses)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training Loss')\n",
        "    plt.grid(True)\n",
        "    plt.savefig('results/training_loss.png')\n",
        "    plt.show()\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prepare_dataset"
      },
      "outputs": [],
      "source": [
        "# Prepare dataset directories and download sample images\n",
        "def prepare_sample_dataset(num_images=5):\n",
        "    \"\"\"Prepare a sample dataset for training\"\"\"\n",
        "    # Create directories\n",
        "    os.makedirs(\"dataset/content\", exist_ok=True)\n",
        "    os.makedirs(\"dataset/style\", exist_ok=True)\n",
        "\n",
        "    # Sample content image URLs\n",
        "    content_urls = [\n",
        "        \"https://images.pexels.com/photos/2559941/pexels-photo-2559941.jpeg\",\n",
        "        \"https://images.pexels.com/photos/1563256/pexels-photo-1563256.jpeg\",\n",
        "        \"https://images.pexels.com/photos/1366630/pexels-photo-1366630.jpeg\",\n",
        "        \"https://images.pexels.com/photos/1366909/pexels-photo-1366909.jpeg\",\n",
        "        \"https://images.pexels.com/photos/1366919/pexels-photo-1366919.jpeg\"\n",
        "    ]\n",
        "\n",
        "    # Sample style image URLs\n",
        "    style_urls = [\n",
        "        \"https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg/1280px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg\",\n",
        "        \"https://upload.wikimedia.org/wikipedia/commons/thumb/c/cc/Grant_Wood_-_American_Gothic_-_Google_Art_Project.jpg/1280px-Grant_Wood_-_American_Gothic_-_Google_Art_Project.jpg\",\n",
        "        \"https://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Picasso_Three_Musicians_MoMA.jpg/1280px-Picasso_Three_Musicians_MoMA.jpg\",\n",
        "        \"https://upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Vassily_Kandinsky%2C_1913_-_Composition_7.jpg/1280px-Vassily_Kandinsky%2C_1913_-_Composition_7.jpg\",\n",
        "        \"https://upload.wikimedia.org/wikipedia/commons/thumb/a/a5/Tsunami_by_hokusai_19th_century.jpg/1280px-Tsunami_by_hokusai_19th_century.jpg\"\n",
        "    ]\n",
        "\n",
        "    # Download content images\n",
        "    print(\"Downloading content images...\")\n",
        "    for i, url in enumerate(content_urls[:num_images]):\n",
        "        try:\n",
        "            response = requests.get(url, timeout=10)\n",
        "            if response.status_code == 200:\n",
        "                with open(f\"dataset/content/content_{i+1}.jpg\", \"wb\") as f:\n",
        "                    f.write(response.content)\n",
        "                print(f\"Downloaded content image {i+1}/{num_images}\")\n",
        "            else:\n",
        "                print(f\"Failed to download content image {i+1}: HTTP {response.status_code}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error downloading content image {i+1}: {e}\")\n",
        "\n",
        "    # Download style images\n",
        "    print(\"\\nDownloading style images...\")\n",
        "    for i, url in enumerate(style_urls[:num_images]):\n",
        "        try:\n",
        "            response = requests.get(url, timeout=10)\n",
        "            if response.status_code == 200:\n",
        "                with open(f\"dataset/style/style_{i+1}.jpg\", \"wb\") as f:\n",
        "                    f.write(response.content)\n",
        "                print(f\"Downloaded style image {i+1}/{num_images}\")\n",
        "            else:\n",
        "                print(f\"Failed to download style image {i+1}: HTTP {response.status_code}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error downloading style image {i+1}: {e}\")\n",
        "\n",
        "    print(\"\\nSample dataset prepared.\")\n",
        "    return \"dataset/content\", \"dataset/style\"\n",
        "\n",
        "# Uncomment to prepare a sample dataset\n",
        "# content_dir, style_dir = prepare_sample_dataset(num_images=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upload_dataset"
      },
      "outputs": [],
      "source": [
        "# Upload your own dataset\n",
        "def upload_dataset():\n",
        "    \"\"\"Upload custom dataset for training\"\"\"\n",
        "    # Create directories\n",
        "    os.makedirs(\"dataset/content\", exist_ok=True)\n",
        "    os.makedirs(\"dataset/style\", exist_ok=True)\n",
        "\n",
        "    print(\"Please upload content images (you can select multiple files):\")\n",
        "    content_files = files.upload()\n",
        "\n",
        "    # Save content images\n",
        "    for filename, content in content_files.items():\n",
        "        with open(f\"dataset/content/{filename}\", \"wb\") as f:\n",
        "            f.write(content)\n",
        "\n",
        "    print(f\"\\nUploaded {len(content_files)} content images.\")\n",
        "\n",
        "    print(\"\\nPlease upload style images (you can select multiple files):\")\n",
        "    style_files = files.upload()\n",
        "\n",
        "    # Save style images\n",
        "    for filename, content in style_files.items():\n",
        "        with open(f\"dataset/style/{filename}\", \"wb\") as f:\n",
        "            f.write(content)\n",
        "\n",
        "    print(f\"\\nUploaded {len(style_files)} style images.\")\n",
        "\n",
        "    return \"dataset/content\", \"dataset/style\"\n",
        "\n",
        "# Uncomment to upload your own dataset\n",
        "# content_dir, style_dir = upload_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_training"
      },
      "outputs": [],
      "source": [
        "# Run training with interactive parameters\n",
        "def interactive_training():\n",
        "    \"\"\"Interactive training with parameter selection\"\"\"\n",
        "    # Create widgets for dataset selection\n",
        "    dataset_options = widgets.RadioButtons(\n",
        "        options=['Use sample dataset', 'Upload my own dataset'],\n",
        "        description='Dataset:',\n",
        "        disabled=False\n",
        "    )\n",
        "\n",
        "    # Create widgets for training parameters\n",
        "    num_epochs_slider = widgets.IntSlider(\n",
        "        value=20,\n",
        "        min=5,\n",
        "        max=100,\n",
        "        step=5,\n",
        "        description='Epochs:',\n",
        "        disabled=False\n",
        "    )\n",
        "\n",
        "    batch_size_slider = widgets.IntSlider(\n",
        "        value=2,\n",
        "        min=1,\n",
        "        max=8,\n",
        "        step=1,\n",
        "        description='Batch Size:',\n",
        "        disabled=False\n",
        "    )\n",
        "\n",
        "    content_weight_slider = widgets.FloatSlider(\n",
        "        value=1.0,\n",
        "        min=0.1,\n",
        "        max=10.0,\n",
        "        step=0.1,\n",
        "        description='Content Weight:',\n",
        "        disabled=False\n",
        "    )\n",
        "\n",
        "    style_weight_slider = widgets.FloatSlider(\n",
        "        value=10.0,\n",
        "        min=1.0,\n",
        "        max=50.0,\n",
        "        step=1.0,\n",
        "        description='Style Weight:',\n",
        "        disabled=False\n",
        "    )\n",
        "\n",
        "    start_button = widgets.Button(\n",
        "        description='Start Training',\n",
        "        disabled=False,\n",
        "        button_style='success',\n",
        "        tooltip='Click to start training'\n",
        "    )\n",
        "\n",
        "    output = widgets.Output()\n",
        "\n",
        "    # Display widgets\n",
        "    display(dataset_options)\n",
        "    display(widgets.HBox([num_epochs_slider, batch_size_slider]))\n",
        "    display(widgets.HBox([content_weight_slider, style_weight_slider]))\n",
        "    display(start_button)\n",
        "    display(output)\n",
        "\n",
        "    # Define button click handler\n",
        "    def on_button_clicked(b):\n",
        "        with output:\n",
        "            clear_output()\n",
        "\n",
        "            # Get dataset\n",
        "            if dataset_options.value == 'Use sample dataset':\n",
        "                content_dir, style_dir = prepare_sample_dataset(num_images=3)\n",
        "            else:\n",
        "                content_dir, style_dir = upload_dataset()\n",
        "\n",
        "            # Get training parameters\n",
        "            num_epochs = num_epochs_slider.value\n",
        "            batch_size = batch_size_slider.value\n",
        "            content_weight = content_weight_slider.value\n",
        "            style_weight = style_weight_slider.value\n",
        "\n",
        "            print(f\"Starting training with the following parameters:\")\n",
        "            print(f\"- Number of epochs: {num_epochs}\")\n",
        "            print(f\"- Batch size: {batch_size}\")\n",
        "            print(f\"- Content weight: {content_weight}\")\n",
        "            print(f\"- Style weight: {style_weight}\")\n",
        "            print(f\"- Content directory: {content_dir}\")\n",
        "            print(f\"- Style directory: {style_dir}\")\n",
        "            print(\"\\n\")\n",
        "\n",
        "            # Run training\n",
        "            trained_model = train_model(\n",
        "                content_dir=content_dir,\n",
        "                style_dir=style_dir,\n",
        "                num_epochs=num_epochs,\n",
        "                batch_size=batch_size,\n",
        "                content_weight=content_weight,\n",
        "                style_weight=style_weight,\n",
        "                checkpoint_interval=5,\n",
        "                image_size=256\n",
        "            )\n",
        "\n",
        "            print(\"\\nTraining complete! You can now use the trained model for style transfer.\")\n",
        "\n",
        "    # Connect button to handler\n",
        "    start_button.on_click(on_button_clicked)\n",
        "\n",
        "# Uncomment to run interactive training\n",
        "# interactive_training()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "performance"
      },
      "source": [
        "## 7. Performance Analysis: TPU vs. CPU vs. GPU <a name=\"performance\"></a>\n",
        "\n",
        "In this section, we'll compare the performance of the StripedHyena neural style transfer model on different hardware accelerators: TPU, GPU, and CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "performance_comparison"
      },
      "outputs": [],
      "source": [
        "# Performance comparison function\n",
        "def compare_performance(content_img, style_img, image_sizes=[256, 512, 1024]):\n",
        "    \"\"\"Compare performance of style transfer on different devices and image sizes\"\"\"\n",
        "    results = []\n",
        "\n",
        "    # Check available devices\n",
        "    available_devices = []\n",
        "    device_names = []\n",
        "\n",
        "    if IS_TPU_AVAILABLE:\n",
        "        available_devices.append(xm.xla_device())\n",
        "        device_names.append(\"TPU\")\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        available_devices.append(torch.device(\"cuda\"))\n",
        "        device_names.append(f\"GPU ({torch.cuda.get_device_name(0)})\")\n",
        "\n",
        "    available_devices.append(torch.device(\"cpu\"))\n",
        "    device_names.append(\"CPU\")\n",
        "\n",
        "    print(f\"Testing performance on {len(available_devices)} devices: {', '.join(device_names)}\")\n",
        "\n",
        "    # Test each device and image size\n",
        "    for size in image_sizes:\n",
        "        print(f\"\\nTesting with image size: {size}x{size}\")\n",
        "\n",
        "        # Resize images\n",
        "        content_resized = content_img.resize((size, size), Image.LANCZOS)\n",
        "        style_resized = style_img.resize((size, size), Image.LANCZOS)\n",
        "\n",
        "        for device, device_name in zip(available_devices, device_names):\n",
        "            print(f\"  Testing on {device_name}...\")\n",
        "\n",
        "            # Initialize model on device\n",
        "            model = StripedHyenaStyleTransfer().to(device)\n",
        "            model.eval()\n",
        "\n",
        "            # Preprocess images\n",
        "            transform = torchvision.transforms.Compose([\n",
        "                torchvision.transforms.ToTensor(),\n",
        "                torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "            ])\n",
        "\n",
        "            content_tensor = transform(content_resized).unsqueeze(0).to(device)\n",
        "            style_tensor = transform(style_resized).unsqueeze(0).to(device)\n",
        "\n",
        "            # Warm-up run\n",
        "            with torch.no_grad():\n",
        "                _ = model(content_tensor, style_tensor)\n",
        "                if device_name == \"TPU\":\n",
        "                    xm.mark_step()\n",
        "\n",
        "            # Timed run\n",
        "            torch.cuda.synchronize() if device_name.startswith(\"GPU\") else None\n",
        "            start_time = time.time()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for _ in range(3):  # Run multiple times for more accurate timing\n",
        "                    _ = model(content_tensor, style_tensor)\n",
        "                    if device_name == \"TPU\":\n",
        "                        xm.mark_step()\n",
        "\n",
        "            torch.cuda.synchronize() if device_name.startswith(\"GPU\") else None\n",
        "            end_time = time.time()\n",
        "\n",
        "            # Calculate average time\n",
        "            avg_time = (end_time - start_time) / 3\n",
        "            print(f\"    Average time: {avg_time:.4f} seconds\")\n",
        "\n",
        "            results.append({\n",
        "                \"device\": device_name,\n",
        "                \"image_size\": size,\n",
        "                \"time\": avg_time\n",
        "            })\n",
        "\n",
        "    # Plot results\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    for device_name in device_names:\n",
        "        device_results = [r for r in results if r[\"device\"] == device_name]\n",
        "        sizes = [r[\"image_size\"] for r in device_results]\n",
        "        times = [r[\"time\"] for r in device_results]\n",
        "        plt.plot(sizes, times, marker='o', label=device_name)\n",
        "\n",
        "    plt.xlabel('Image Size (pixels)')\n",
        "    plt.ylabel('Processing Time (seconds)')\n",
        "    plt.title('StripedHyena Neural Style Transfer Performance Comparison')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.savefig('results/performance_comparison.png')\n",
        "    plt.show()\n",
        "\n",
        "    return results\n",
        "\n",
        "# Uncomment to run performance comparison\n",
        "# try:\n",
        "#     performance_results = compare_performance(content_img, style_img, image_sizes=[256, 512])\n",
        "# except NameError:\n",
        "#     print(\"Please load content and style images first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "export"
      },
      "source": [
        "## 8. Exporting Your Model <a name=\"export\"></a>\n",
        "\n",
        "In this section, we'll show how to export your trained model for deployment or sharing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "export_model"
      },
      "outputs": [],
      "source": [
        "# Export model function\n",
        "def export_model(model_path=\"checkpoints/model_final.pth\"):\n",
        "    \"\"\"Export the trained model for deployment\"\"\"\n",
        "    # Create export directory\n",
        "    os.makedirs(\"export\", exist_ok=True)\n",
        "\n",
        "    # Check if model exists\n",
        "    if not os.path.exists(model_path):\n",
        "        print(f\"Model file {model_path} not found. Please train a model first.\")\n",
        "        return\n",
        "\n",
        "    # Load model\n",
        "    model = StripedHyenaStyleTransfer().to(device)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    # Export to TorchScript\n",
        "    print(\"Exporting model to TorchScript...\")\n",
        "    example_content = torch.randn(1, 3, 256, 256).to(device)\n",
        "    example_style = torch.randn(1, 3, 256, 256).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        traced_model = torch.jit.trace(model, (example_content, example_style))\n",
        "        traced_model.save(\"export/model_traced.pt\")\n",
        "\n",
        "    print(\"Model exported to export/model_traced.pt\")\n",
        "\n",
        "    # Create a simple inference script\n",
        "    inference_script = \"\"\"\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "\n",
        "def load_image(image_path, max_size=512):\n",
        "    \"\"\"Load and preprocess an image\"\"\"\n",
        "    img = Image.open(image_path).convert('RGB')\n",
        "\n",
        "    # Resize while maintaining aspect ratio\n",
        "    if max(img.size) > max_size:\n",
        "        ratio = max_size / max(img.size)\n",
        "        new_size = (int(img.size[0] * ratio), int(img.size[1] * ratio))\n",
        "        img = img.resize(new_size, Image.LANCZOS)\n",
        "\n",
        "    return img\n",
        "\n",
        "def preprocess_image(img):\n",
        "    \"\"\"Convert PIL image to tensor for model input\"\"\"\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    img_tensor = transform(img).unsqueeze(0)\n",
        "    return img_tensor\n",
        "\n",
        "def deprocess_image(tensor):\n",
        "    \"\"\"Convert tensor to PIL image for display\"\"\"\n",
        "    tensor = tensor.squeeze(0).detach().clone()\n",
        "    tensor = tensor.clamp(0, 1)\n",
        "\n",
        "    # Convert to PIL image\n",
        "    img = transforms.ToPILImage()(tensor)\n",
        "    return img\n",
        "\n",
        "def apply_style_transfer(model_path, content_path, style_path, output_path, alpha=0.8):\n",
        "    \"\"\"Apply style transfer using the exported model\"\"\"\n",
        "    # Load model\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = torch.jit.load(model_path).to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Load images\n",
        "    content_img = load_image(content_path)\n",
        "    style_img = load_image(style_path)\n",
        "\n",
        "    # Preprocess images\n",
        "    content_tensor = preprocess_image(content_img).to(device)\n",
        "    style_tensor = preprocess_image(style_img).to(device)\n",
        "\n",
        "    # Apply style transfer\n",
        "    with torch.no_grad():\n",
        "        output_tensor = model(content_tensor, style_tensor)\n",
        "\n",
        "    # Save result\n",
        "    output_img = deprocess_image(output_tensor)\n",
        "    output_img.save(output_path)\n",
        "    print(f\"Stylized image saved to {output_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser(description=\"Apply neural style transfer using StripedHyena model\")\n",
        "    parser.add_argument(\"--model\", type=str, default=\"model_traced.pt\", help=\"Path to the exported model\")\n",
        "    parser.add_argument(\"--content\", type=str, required=True, help=\"Path to the content image\")\n",
        "    parser.add_argument(\"--style\", type=str, required=True, help=\"Path to the style image\")\n",
        "    parser.add_argument(\"--output\", type=str, default=\"stylized_output.jpg\", help=\"Path to save the output image\")\n",
        "    parser.add_argument(\"--alpha\", type=float, default=0.8, help=\"Content weight (0.0 to 1.0)\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    apply_style_transfer(args.model, args.content, args.style, args.output, args.alpha)\n",
        "\"\"\"\n",
        "\n",
        "    with open(\"export/inference.py\", \"w\") as f:\n",
        "        f.write(inference_script)\n",
        "\n",
        "    print(\"Inference script saved to export/inference.py\")\n",
        "\n",
        "    # Create a README file\n",
        "    readme = \"\"\"\n",
        "# StripedHyena Neural Style Transfer\n",
        "\n",
        "This package contains a trained StripedHyena neural style transfer model.\n",
        "\n",
        "## Contents\n",
        "\n",
        "- `model_traced.pt`: The exported TorchScript model\n",
        "- `inference.py`: A script for applying style transfer\n",
        "\n",
        "## Requirements\n",
        "\n",
        "- Python 3.8+\n",
        "- PyTorch 2.0+\n",
        "- torchvision\n",
        "- Pillow\n",
        "\n",
        "## Usage\n",
        "\n",
        "```bash\n",
        "python inference.py --content path/to/content.jpg --style path/to/style.jpg --output stylized_output.jpg\n",
        "```\n",
        "\n",
        "## Parameters\n",
        "\n",
        "- `--model`: Path to the exported model (default: model_traced.pt)\n",
        "- `--content`: Path to the content image\n",
        "- `--style`: Path to the style image\n",
        "- `--output`: Path to save the output image (default: stylized_output.jpg)\n",
        "- `--alpha`: Content weight (0.0 to 1.0, default: 0.8)\n",
        "\"\"\"\n",
        "\n",
        "    with open(\"export/README.md\", \"w\") as f:\n",
        "        f.write(readme)\n",
        "\n",
        "    print(\"README file saved to export/README.md\")\n",
        "\n",
        "    # Create a zip file\n",
        "    !zip -r export/striped_hyena_style_transfer.zip export/model_traced.pt export/inference.py export/README.md\n",
        "\n",
        "    print(\"\\nModel exported successfully! You can download the zip file from the Files tab.\")\n",
        "    return \"export/striped_hyena_style_transfer.zip\"\n",
        "\n",
        "# Uncomment to export a trained model\n",
        "# export_path = export_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusion"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this notebook, we've implemented and explored the StripedHyena neural style transfer model, optimized for Google Colab's TPU environment. We've covered:\n",
        "\n",
        "1. The architecture and components of the StripedHyena model\n",
        "2. Implementation of the model for neural style transfer\n",
        "3. Basic and advanced style transfer demos\n",
        "4. Training on custom datasets\n",
        "5. Performance comparison across different hardware accelerators\n",
        "6. Exporting the model for deployment\n",
        "\n",
        "The StripedHyena architecture, with its hybrid approach combining rotary attention and gated convolutions, offers an efficient and effective solution for neural style transfer tasks. Its memory efficiency and linear scaling properties make it particularly well-suited for processing high-resolution images.\n",
        "\n",
        "Feel free to experiment with different content and style images, adjust the parameters, and train the model on your own datasets to achieve unique and personalized style transfer results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download_export"
      },
      "outputs": [],
      "source": [
        "# Download the exported model\n",
        "def download_export(export_path=\"export/striped_hyena_style_transfer.zip\"):\n",
        "    \"\"\"Download the exported model\"\"\"\n",
        "    from google.colab import files\n",
        "\n",
        "    if os.path.exists(export_path):\n",
        "        files.download(export_path)\n",
        "        print(f\"Downloaded {export_path}\")\n",
        "    else:\n",
        "        print(f\"Export file {export_path} not found. Please export the model first.\")\n",
        "\n",
        "# Uncomment to download the exported model\n",
        "# download_export()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required packages\n",
        "!pip install torch torchvision matplotlib numpy pillow tqdm requests\n",
        "\n",
        "# If TPU is available, install PyTorch XLA\n",
        "if IS_TPU_AVAILABLE:\n",
        "    !pip install cloud-tpu-client==0.10 torch_xla[tpu]==2.0 -f https://storage.googleapis.com/libtpu-releases/index.html\n",
        "\n",
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import time\n",
        "import random\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "from tqdm import tqdm # Ensure tqdm is imported\n",
        "\n",
        "# Assuming IS_TPU_AVAILABLE, device, xm are defined from previous cells\n",
        "# (setup_device cell)\n",
        "\n",
        "# If IS_TPU_AVAILABLE:\n",
        "# import torch_xla.core.xla_model as xm\n",
        "# device = xm.xla_device()\n",
        "# else:\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Assuming RotaryEmbedding, GatedConv2d, StripedHyenaBlock, StripedHyenaStyleTransfer\n",
        "# ContentLoss, StyleLoss, TVLoss are defined from previous cells\n",
        "\n",
        "# Assuming StyleTransferDataset, get_dataloader, train_model,\n",
        "# prepare_sample_dataset, upload_dataset are defined from previous cells\n",
        "\n",
        "# Import widgets and display utilities\n",
        "from google.colab import files\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# Run training with interactive parameters\n",
        "def interactive_training():\n",
        "    \"\"\"Interactive training with parameter selection\"\"\"\n",
        "\n",
        "    print(\"Setup your training session:\")\n",
        "\n",
        "    # Create widgets for dataset selection\n",
        "    dataset_options = widgets.RadioButtons(\n",
        "        options=['Use sample dataset', 'Upload my own dataset', 'Use existing Colab paths'],\n",
        "        description='Dataset Source:',\n",
        "        value='Use sample dataset', # Default value\n",
        "        disabled=False,\n",
        "        style={'description_width': 'initial'}\n",
        "    )\n",
        "\n",
        "    # Widgets for Colab paths (initially hidden or shown based on dataset_options)\n",
        "    content_colab_path_input = widgets.Text(\n",
        "        value='dataset/content',  # Default example path\n",
        "        placeholder='e.g., /content/my_content_data or /content/drive/MyDrive/content_folder',\n",
        "        description='Content Dir Path:',\n",
        "        disabled=False,\n",
        "        layout=widgets.Layout(width='auto'),\n",
        "        style={'description_width': 'initial'}\n",
        "    )\n",
        "    style_colab_path_input = widgets.Text(\n",
        "        value='dataset/style',  # Default example path\n",
        "        placeholder='e.g., /content/my_style_data or /content/drive/MyDrive/style_folder',\n",
        "        description='Style Dir Path:',\n",
        "        disabled=False,\n",
        "        layout=widgets.Layout(width='auto'),\n",
        "        style={'description_width': 'initial'}\n",
        "    )\n",
        "    colab_paths_box = widgets.VBox([\n",
        "        widgets.HTML(\"<b>If using existing Colab paths, ensure they are correct:</b>\"),\n",
        "        content_colab_path_input,\n",
        "        style_colab_path_input\n",
        "    ])\n",
        "\n",
        "    # Function to handle visibility of Colab path inputs\n",
        "    dataset_interaction_output_area = widgets.Output()\n",
        "    def handle_dataset_option_change(change):\n",
        "        with dataset_interaction_output_area:\n",
        "            clear_output(wait=True)\n",
        "            if change.new == 'Use existing Colab paths':\n",
        "                display(colab_paths_box)\n",
        "            elif change.new == 'Upload my own dataset':\n",
        "                print(\"INFO: When you click 'Start Training', you will be prompted to upload content and style images.\")\n",
        "                print(\"      These will be saved to 'dataset/content' and 'dataset/style' respectively.\")\n",
        "            elif change.new == 'Use sample dataset':\n",
        "                print(\"INFO: When you click 'Start Training', sample images will be downloaded to 'dataset/content' and 'dataset/style'.\")\n",
        "\n",
        "    dataset_options.observe(handle_dataset_option_change, names='value')\n",
        "\n",
        "    # Create widgets for training parameters\n",
        "    num_epochs_slider = widgets.IntSlider(\n",
        "        value=20, min=5, max=100, step=5, description='Epochs:', disabled=False,\n",
        "        style={'description_width': 'initial'}\n",
        "    )\n",
        "    batch_size_slider = widgets.IntSlider(\n",
        "        value=2, min=1, max=8, step=1, description='Batch Size:', disabled=False,\n",
        "        style={'description_width': 'initial'}\n",
        "    )\n",
        "    content_weight_slider = widgets.FloatSlider(\n",
        "        value=1.0, min=0.1, max=10.0, step=0.1, description='Content W.:', disabled=False,\n",
        "        readout_format='.1f', style={'description_width': 'initial'}\n",
        "    )\n",
        "    style_weight_slider = widgets.FloatSlider(\n",
        "        value=10.0, min=1.0, max=100.0, step=1.0, description='Style W.:', disabled=False,\n",
        "        readout_format='.1f', style={'description_width': 'initial'}\n",
        "    )\n",
        "\n",
        "    start_button = widgets.Button(\n",
        "        description='Start Training', disabled=False, button_style='success',\n",
        "        tooltip='Click to start training', icon='play'\n",
        "    )\n",
        "\n",
        "    training_output_area = widgets.Output() # For logs and plots from training\n",
        "\n",
        "    # Display widgets\n",
        "    display(widgets.VBox([\n",
        "        dataset_options,\n",
        "        dataset_interaction_output_area, # Placeholder for colab_paths_box or messages\n",
        "        widgets.HTML(\"<hr><b>Training Hyperparameters:</b>\"),\n",
        "        widgets.HBox([num_epochs_slider, batch_size_slider]),\n",
        "        widgets.HBox([content_weight_slider, style_weight_slider]),\n",
        "        start_button\n",
        "    ]))\n",
        "    display(training_output_area)\n",
        "\n",
        "    # Call handler initially to set correct visibility for the default option\n",
        "    handle_dataset_option_change({'new': dataset_options.value})\n",
        "\n",
        "    # Define button click handler\n",
        "    def on_button_clicked(b):\n",
        "        with training_output_area:\n",
        "            clear_output(wait=True) # Clear previous training logs\n",
        "\n",
        "            content_dir = \"\"\n",
        "            style_dir = \"\"\n",
        "\n",
        "            # Get dataset\n",
        "            if dataset_options.value == 'Use sample dataset':\n",
        "                print(\"Preparing sample dataset...\")\n",
        "                content_dir, style_dir = prepare_sample_dataset(num_images=3) # Using a small number for quick demo\n",
        "            elif dataset_options.value == 'Upload my own dataset':\n",
        "                print(\"Please follow the prompts to upload your dataset.\")\n",
        "                # upload_dataset will print its own prompts\n",
        "                content_dir, style_dir = upload_dataset()\n",
        "            elif dataset_options.value == 'Use existing Colab paths':\n",
        "                content_dir = content_colab_path_input.value\n",
        "                style_dir = style_colab_path_input.value\n",
        "                print(f\"Using existing Colab paths:\\n  Content: {content_dir}\\n  Style:   {style_dir}\")\n",
        "                # Validate paths\n",
        "                if not os.path.exists(content_dir):\n",
        "                    print(f\"ERROR: Content directory '{content_dir}' not found. Please check the path.\")\n",
        "                    return\n",
        "                if not os.path.isdir(content_dir):\n",
        "                    print(f\"ERROR: Content path '{content_dir}' is not a directory.\")\n",
        "                    return\n",
        "                if not os.path.exists(style_dir):\n",
        "                    print(f\"ERROR: Style directory '{style_dir}' not found. Please check the path.\")\n",
        "                    return\n",
        "                if not os.path.isdir(style_dir):\n",
        "                    print(f\"ERROR: Style path '{style_dir}' is not a directory.\")\n",
        "                    return\n",
        "            else:\n",
        "                print(\"ERROR: Invalid dataset option selected.\")\n",
        "                return\n",
        "\n",
        "            if not content_dir or not style_dir:\n",
        "                print(\"ERROR: Content or style directory not set. Cannot start training.\")\n",
        "                return\n",
        "\n",
        "            # Get training parameters\n",
        "            num_epochs = num_epochs_slider.value\n",
        "            batch_size = batch_size_slider.value\n",
        "            content_weight = content_weight_slider.value\n",
        "            style_weight = style_weight_slider.value\n",
        "\n",
        "            print(f\"\\nStarting training with the following parameters:\")\n",
        "            print(f\"- Number of epochs: {num_epochs}\")\n",
        "            print(f\"- Batch size: {batch_size}\")\n",
        "            print(f\"- Content weight: {content_weight}\")\n",
        "            print(f\"- Style weight: {style_weight}\")\n",
        "            print(f\"- Content directory: {content_dir}\")\n",
        "            print(f\"- Style directory: {style_dir}\")\n",
        "            print(f\"- Device: {device}\")\n",
        "            print(\"\\n\")\n",
        "\n",
        "            # Run training\n",
        "            try:\n",
        "                trained_model = train_model(\n",
        "                    content_dir=content_dir,\n",
        "                    style_dir=style_dir,\n",
        "                    num_epochs=num_epochs,\n",
        "                    batch_size=batch_size,\n",
        "                    content_weight=content_weight,\n",
        "                    style_weight=style_weight,\n",
        "                    checkpoint_interval=max(1, num_epochs // 4), # Checkpoint a few times\n",
        "                    image_size=256 # Keep fixed for this demo, can be made a widget too\n",
        "                )\n",
        "                print(\"\\nTraining complete! You can now use the trained model (checkpoints/model_final.pth) for style transfer.\")\n",
        "                print(\"The latest model is also available as 'model' in the global scope if you ran training in this session.\")\n",
        "                # Optionally, make the trained_model global or assign it to the 'model' variable used in demos\n",
        "                # global model\n",
        "                # model = trained_model\n",
        "            except ValueError as e:\n",
        "                print(f\"ERROR during dataset loading or training: {e}\")\n",
        "                print(\"Please check if your dataset directories are correctly populated with images.\")\n",
        "            except Exception as e:\n",
        "                print(f\"An unexpected error occurred during training: {e}\")\n",
        "                import traceback\n",
        "                traceback.print_exc()\n",
        "\n",
        "    # Connect button to handler\n",
        "    start_button.on_click(on_button_clicked)\n",
        "\n",
        "# To run the interactive training:\n",
        "interactive_training()"
      ],
      "metadata": {
        "id": "U-siEwiVSsXP"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}