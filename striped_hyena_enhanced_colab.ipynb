{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thePegasusai/stripedhyena/blob/main/striped_hyena_enhanced_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "introduction"
      },
      "source": [
        "# StripedHyena Neural Style Transfer - Enhanced TPU Version\n",
        "\n",
        "This notebook provides an enhanced implementation of neural style transfer using the StripedHyena architecture developed by Liquid AI. The implementation is specifically optimized for Google Colab's TPU environment.\n",
        "\n",
        "## What's New in This Enhanced Version\n",
        "\n",
        "- **Detailed Architecture Explanations**: In-depth explanations of the StripedHyena architecture components\n",
        "- **Interactive Examples**: User-friendly interface for experimenting with different content and style images\n",
        "- **Custom Dataset Training**: Support for training on your own dataset of images\n",
        "- **TPU Optimizations**: Performance enhancements specifically for Google Colab's TPU environment\n",
        "- **Visualization Tools**: Better visualization of the style transfer process and results\n",
        "- **Hyperparameter Tuning**: Interactive controls for adjusting style transfer parameters\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. [Setup and Dependencies](#setup)\n",
        "2. [StripedHyena Architecture Overview](#architecture)\n",
        "3. [Model Implementation](#implementation)\n",
        "4. [Basic Style Transfer Demo](#basic_demo)\n",
        "5. [Advanced Style Transfer with Parameter Tuning](#advanced_demo)\n",
        "6. [Training on Custom Datasets](#custom_training)\n",
        "7. [Performance Analysis: TPU vs. CPU vs. GPU](#performance)\n",
        "8. [Exporting Your Model](#export)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## 1. Setup and Dependencies <a name=\"setup\"></a>\n",
        "\n",
        "First, let's set up our environment and install the necessary dependencies. This notebook is designed to work with TPUs, but will fall back to GPU or CPU if TPUs are not available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "check_tpu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa0475a1-4e79-4354-a8a8-519358e68c9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "DEVICE DETECTION\n",
            "==================================================\n",
            "COLAB_TPU_ADDR environment variable not found\n",
            "XLA device detected: xla:0\n",
            "Number of TPU cores: 1\n",
            "âœ… TPU is available and accessible via torch_xla!\n",
            "ðŸš€ Using TPU: xla:0\n",
            "\n",
            "==================================================\n",
            "ADDITIONAL TPU SETUP (if using TPU)\n",
            "==================================================\n",
            "\n",
            "To use TPU effectively in your training loop, remember to:\n",
            "\n",
            "1. Move your model to TPU:\n",
            "   model = model.to(device)\n",
            "\n",
            "2. Use xm.optimizer_step() instead of optimizer.step():\n",
            "   import torch_xla.core.xla_model as xm\n",
            "   xm.optimizer_step(optimizer)\n",
            "\n",
            "3. Use xm.mark_step() for gradient synchronization:\n",
            "   xm.mark_step()\n",
            "\n",
            "4. For multi-core TPU training, use xm.spawn():\n",
            "   import torch_xla.distributed.parallel_loader as pl\n",
            "        \n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "\n",
        "def check_tpu_availability():\n",
        "    \"\"\"Check if TPU is available and properly configured\"\"\"\n",
        "\n",
        "    # Method 1: Check environment variables\n",
        "    tpu_address = os.environ.get('COLAB_TPU_ADDR')\n",
        "    if tpu_address:\n",
        "        print(f\"TPU address found: {tpu_address}\")\n",
        "        IS_TPU_AVAILABLE = True\n",
        "    else:\n",
        "        print(\"COLAB_TPU_ADDR environment variable not found\")\n",
        "        IS_TPU_AVAILABLE = False\n",
        "\n",
        "    # Method 2: Try to import and detect TPU using torch_xla\n",
        "    try:\n",
        "        import torch_xla\n",
        "        import torch_xla.core.xla_model as xm\n",
        "\n",
        "        # Check if XLA devices are available\n",
        "        device = xm.xla_device()\n",
        "        print(f\"XLA device detected: {device}\")\n",
        "\n",
        "        # Get TPU device count (using new API to avoid deprecation warning)\n",
        "        try:\n",
        "            import torch_xla.runtime as xr\n",
        "            device_count = xr.world_size()\n",
        "        except (ImportError, AttributeError):\n",
        "            # Fallback to deprecated method if new one isn't available\n",
        "            device_count = xm.xrt_world_size()\n",
        "        print(f\"Number of TPU cores: {device_count}\")\n",
        "\n",
        "        if device_count > 0:\n",
        "            IS_TPU_AVAILABLE = True\n",
        "            print(\"âœ… TPU is available and accessible via torch_xla!\")\n",
        "        else:\n",
        "            IS_TPU_AVAILABLE = False\n",
        "            print(\"âŒ TPU cores not detected\")\n",
        "\n",
        "    except ImportError:\n",
        "        print(\"torch_xla not installed - installing now...\")\n",
        "        # Install torch_xla for TPU support\n",
        "        os.system('pip install torch_xla[tpu] -f https://storage.googleapis.com/libtpu-releases/index.html')\n",
        "        print(\"Please restart runtime after installation\")\n",
        "        IS_TPU_AVAILABLE = False\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing TPU via torch_xla: {e}\")\n",
        "        IS_TPU_AVAILABLE = False\n",
        "\n",
        "    return IS_TPU_AVAILABLE\n",
        "\n",
        "def check_gpu_availability():\n",
        "    \"\"\"Check GPU availability\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        gpu_count = torch.cuda.device_count()\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        print(f\"âœ… GPU is available: {gpu_name}\")\n",
        "        print(f\"Number of GPUs: {gpu_count}\")\n",
        "        return True\n",
        "    else:\n",
        "        print(\"âŒ GPU is not available\")\n",
        "        return False\n",
        "\n",
        "def setup_device():\n",
        "    \"\"\"Setup the appropriate device for training\"\"\"\n",
        "\n",
        "    print(\"=\" * 50)\n",
        "    print(\"DEVICE DETECTION\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    # Check TPU first\n",
        "    tpu_available = check_tpu_availability()\n",
        "\n",
        "    if tpu_available:\n",
        "        try:\n",
        "            import torch_xla.core.xla_model as xm\n",
        "            device = xm.xla_device()\n",
        "            print(f\"ðŸš€ Using TPU: {device}\")\n",
        "            return device, \"tpu\"\n",
        "        except:\n",
        "            print(\"Failed to initialize TPU device\")\n",
        "\n",
        "    # Check GPU if TPU not available\n",
        "    gpu_available = check_gpu_availability()\n",
        "\n",
        "    if gpu_available:\n",
        "        device = torch.device(\"cuda\")\n",
        "        print(f\"ðŸš€ Using GPU: {device}\")\n",
        "        return device, \"gpu\"\n",
        "\n",
        "    # Fallback to CPU\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"ðŸš€ Using CPU\")\n",
        "    return device, \"cpu\"\n",
        "\n",
        "# Run the detection\n",
        "if __name__ == \"__main__\":\n",
        "    device, device_type = setup_device()\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 50)\n",
        "    print(\"ADDITIONAL TPU SETUP (if using TPU)\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    if device_type == \"tpu\":\n",
        "        print(\"\"\"\n",
        "To use TPU effectively in your training loop, remember to:\n",
        "\n",
        "1. Move your model to TPU:\n",
        "   model = model.to(device)\n",
        "\n",
        "2. Use xm.optimizer_step() instead of optimizer.step():\n",
        "   import torch_xla.core.xla_model as xm\n",
        "   xm.optimizer_step(optimizer)\n",
        "\n",
        "3. Use xm.mark_step() for gradient synchronization:\n",
        "   xm.mark_step()\n",
        "\n",
        "4. For multi-core TPU training, use xm.spawn():\n",
        "   import torch_xla.distributed.parallel_loader as pl\n",
        "        \"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch_xla.core.xla_model as xm\n",
        "import time\n",
        "\n",
        "device = xm.xla_device()\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Test computation on TPU\n",
        "print(\"\\nðŸ§ª Testing TPU computation...\")\n",
        "start_time = time.time()\n",
        "\n",
        "# Create tensors on TPU\n",
        "x = torch.randn(1000, 1000).to(device)\n",
        "y = torch.randn(1000, 1000).to(device)\n",
        "\n",
        "# Perform matrix multiplication\n",
        "result = torch.matmul(x, y)\n",
        "\n",
        "# Important: Mark step to sync computation\n",
        "xm.mark_step()\n",
        "\n",
        "end_time = time.time()\n",
        "print(f\"âœ… Matrix multiplication completed in {end_time - start_time:.4f} seconds\")\n",
        "print(f\"Result shape: {result.shape}\")\n",
        "print(f\"Result device: {result.device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3b-w__tjiX6",
        "outputId": "237e37e9-31dc-4240-cb4a-0281f38c3f90"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: xla:0\n",
            "\n",
            "ðŸ§ª Testing TPU computation...\n",
            "âœ… Matrix multiplication completed in 0.9469 seconds\n",
            "Result shape: torch.Size([1000, 1000])\n",
            "Result device: xla:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "install_dependencies",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0136535-41be-4ef9-95cb-cc4bf68bafc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cpu)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cpu)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.5.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.4.26)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install torch torchvision matplotlib numpy pillow tqdm requests\n",
        "\n",
        "# If TPU is available, install PyTorch XLA\n",
        "if IS_TPU_AVAILABLE:\n",
        "    !pip install cloud-tpu-client==0.10 torch_xla[tpu]==2.0 -f https://storage.googleapis.com/libtpu-releases/index.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "setup_device",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04c66a0f-2a05-47c6-a582-e0d80e9a09d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using TPU: xla:0\n",
            "Device set to: xla:0\n",
            "PyTorch version: 2.6.0+cpu\n",
            "Torchvision version: 0.21.0+cpu\n",
            "Test tensor shape: torch.Size([3, 3])\n",
            "Test tensor device: xla:0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import requests\n",
        "from io import BytesIO\n",
        "import time\n",
        "\n",
        "# Check if TPU is available (you'll need to define IS_TPU_AVAILABLE or check differently)\n",
        "try:\n",
        "    import torch_xla.core.xla_model as xm\n",
        "    IS_TPU_AVAILABLE = True\n",
        "except ImportError:\n",
        "    IS_TPU_AVAILABLE = False\n",
        "\n",
        "# Set up device\n",
        "if IS_TPU_AVAILABLE:\n",
        "    try:\n",
        "        import torch_xla.core.xla_model as xm\n",
        "        device = xm.xla_device()\n",
        "        print(f\"Using TPU: {device}\")\n",
        "    except Exception as e:\n",
        "        print(f\"TPU initialization failed: {e}\")\n",
        "        device = torch.device(\"cpu\")\n",
        "        print(\"Falling back to CPU\")\n",
        "elif torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"Using CPU\")\n",
        "\n",
        "# Set random seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "# Additional setup for different devices\n",
        "if device.type == 'cuda':\n",
        "    torch.cuda.manual_seed(42)\n",
        "    torch.cuda.manual_seed_all(42)  # For multi-GPU setups\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "print(f\"Device set to: {device}\")\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"Torchvision version: {torchvision.__version__}\")\n",
        "\n",
        "# Test tensor creation on the device\n",
        "test_tensor = torch.randn(3, 3).to(device)\n",
        "print(f\"Test tensor shape: {test_tensor.shape}\")\n",
        "print(f\"Test tensor device: {test_tensor.device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "architecture"
      },
      "source": [
        "## 2. StripedHyena Architecture Overview <a name=\"architecture\"></a>\n",
        "\n",
        "The StripedHyena architecture, developed by Liquid AI, is a hybrid neural network architecture that combines rotary (grouped) attention with gated convolutions. This section provides a detailed explanation of the architecture and its components.\n",
        "\n",
        "### 2.1 Key Components of StripedHyena\n",
        "\n",
        "#### Rotary Positional Embeddings\n",
        "Rotary positional embeddings (RoPE) encode spatial information, allowing the model to understand the relative positions of features in both content and style images. This is crucial for maintaining spatial coherence in the stylized output.\n",
        "\n",
        "```\n",
        "def apply_rotary_embeddings(x, freqs):\n",
        "    # x: [batch, seq_len, dim]\n",
        "    # freqs: [seq_len, dim/2]\n",
        "    seq_len = x.shape[1]\n",
        "    x_complex = torch.view_as_complex(x.float().reshape(*x.shape[:-1], -1, 2))\n",
        "    freqs = freqs[:seq_len]\n",
        "    freqs = torch.view_as_complex(freqs.reshape(*freqs.shape[:-1], -1, 2))\n",
        "    x_rotated = x_complex * freqs\n",
        "    x_rotated = torch.view_as_real(x_rotated).flatten(-2)\n",
        "    return x_rotated.type_as(x)\n",
        "```\n",
        "\n",
        "#### Gated Convolutions\n",
        "The architecture employs gated convolutions that adaptively control information flow, allowing the model to selectively apply style features based on content characteristics. This results in more natural-looking style transfers that preserve important content details.\n",
        "\n",
        "```\n",
        "class GatedConv2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels*2, kernel_size, stride, padding)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # Split channels into two parts\n",
        "        h = self.conv(x)\n",
        "        a, b = torch.chunk(h, 2, dim=1)\n",
        "        # Apply gating mechanism\n",
        "        return a * torch.sigmoid(b)\n",
        "```\n",
        "\n",
        "#### Hybrid Attention Mechanism\n",
        "StripedHyena combines local processing (through convolutions) with global context awareness (through attention mechanisms), creating a hybrid approach that captures both fine details and overall style patterns. This hybrid design is particularly effective for style transfer tasks.\n",
        "\n",
        "### 2.2 Advantages for Neural Style Transfer\n",
        "\n",
        "1. **Memory Efficiency**: Processes high-resolution images with significantly lower memory requirements than traditional Transformer models.\n",
        "2. **Linear Scaling**: Computational complexity scales linearly with image size instead of quadratically.\n",
        "3. **Long-range Dependencies**: Efficiently captures relationships between distant parts of the image.\n",
        "4. **Adaptive Processing**: Intelligently applies style based on content characteristics.\n",
        "5. **TPU Compatibility**: Architecture is well-suited for TPU acceleration.\n",
        "\n",
        "### 2.3 Adaptation for 2D Images\n",
        "\n",
        "The original StripedHyena architecture was designed for 1D sequence processing. For neural style transfer, we adapt it to 2D image processing by:\n",
        "\n",
        "1. Converting 2D images to sequences by flattening spatial dimensions\n",
        "2. Applying StripedHyena processing\n",
        "3. Reshaping back to 2D for convolutional processing\n",
        "4. Using skip connections to preserve spatial information\n",
        "\n",
        "This approach allows us to leverage the strengths of the StripedHyena architecture while maintaining the spatial structure necessary for image processing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "implementation"
      },
      "source": [
        "## 3. Model Implementation <a name=\"implementation\"></a>\n",
        "\n",
        "Now, let's implement the StripedHyena neural style transfer model. We'll start with the core components and then build the complete model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rotary_embeddings"
      },
      "outputs": [],
      "source": [
        "# Rotary Positional Embeddings implementation\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "\n",
        "class RotaryEmbedding(nn.Module):\n",
        "    def __init__(self, dim, base=10000):\n",
        "        super().__init__()\n",
        "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
        "        self.register_buffer(\"inv_freq\", inv_freq)\n",
        "        self.seq_len_cached = None\n",
        "        self.cos_cached = None\n",
        "        self.sin_cached = None\n",
        "\n",
        "    def forward(self, x, seq_len=None):\n",
        "        if seq_len is None:\n",
        "            seq_len = x.shape[1]\n",
        "\n",
        "        if seq_len != self.seq_len_cached:\n",
        "            self.seq_len_cached = seq_len\n",
        "            t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n",
        "            freqs = torch.einsum(\"i,j->ij\", t, self.inv_freq)\n",
        "            emb = torch.cat((freqs, freqs), dim=-1).to(x.device)\n",
        "            self.cos_cached = emb.cos()[None, :, None, :]\n",
        "            self.sin_cached = emb.sin()[None, :, None, :]\n",
        "\n",
        "        return self.cos_cached, self.sin_cached\n",
        "\n",
        "# Function to apply rotary embeddings\n",
        "def apply_rotary_pos_emb(q, k, cos, sin):\n",
        "    # Reshape q and k for the rotation\n",
        "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
        "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
        "    return q_embed, k_embed\n",
        "\n",
        "def rotate_half(x):\n",
        "    x1, x2 = x.chunk(2, dim=-1)\n",
        "    return torch.cat((-x2, x1), dim=-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gated_convolutions"
      },
      "outputs": [],
      "source": [
        "# Gated Convolution implementation\n",
        "class GatedConv2d(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels*2, kernel_size, stride, padding)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.conv(x)\n",
        "        a, b = torch.chunk(h, 2, dim=1)\n",
        "        return a * torch.sigmoid(b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyena_block"
      },
      "outputs": [],
      "source": [
        "# StripedHyena Block implementation\n",
        "class StripedHyenaBlock(nn.Module):\n",
        "    def __init__(self, dim, heads=4, dim_head=64, dropout=0.0):\n",
        "        super().__init__()\n",
        "        inner_dim = dim_head * heads\n",
        "        self.heads = heads\n",
        "        self.dim_head = dim_head\n",
        "\n",
        "        # Rotary embeddings\n",
        "        self.rotary_emb = RotaryEmbedding(dim_head)\n",
        "\n",
        "        # Projections for Q, K, V\n",
        "        self.to_q = nn.Linear(dim, inner_dim, bias=False)\n",
        "        self.to_k = nn.Linear(dim, inner_dim, bias=False)\n",
        "        self.to_v = nn.Linear(dim, inner_dim, bias=False)\n",
        "        self.to_out = nn.Linear(inner_dim, dim)\n",
        "\n",
        "        # Gated feed-forward network\n",
        "        self.ff = nn.Sequential(\n",
        "            nn.Linear(dim, dim * 4),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(dim * 4, dim),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        # Layer norms\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "\n",
        "        # Dropout\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Layer norm 1\n",
        "        normed_x = self.norm1(x)\n",
        "\n",
        "        # Self-attention with rotary embeddings\n",
        "        q = self.to_q(normed_x)\n",
        "        k = self.to_k(normed_x)\n",
        "        v = self.to_v(normed_x)\n",
        "\n",
        "        # Reshape for multi-head attention\n",
        "        q = q.view(q.shape[0], q.shape[1], self.heads, self.dim_head)\n",
        "        k = k.view(k.shape[0], k.shape[1], self.heads, self.dim_head)\n",
        "        v = v.view(v.shape[0], v.shape[1], self.heads, self.dim_head)\n",
        "\n",
        "        # Apply rotary embeddings\n",
        "        cos, sin = self.rotary_emb(q, seq_len=q.shape[1])\n",
        "        q, k = apply_rotary_pos_emb(q, k, cos, sin)\n",
        "\n",
        "        # Reshape for attention computation\n",
        "        q = q.transpose(1, 2)  # [batch, heads, seq_len, dim_head]\n",
        "        k = k.transpose(1, 2)\n",
        "        v = v.transpose(1, 2)\n",
        "\n",
        "        # Compute attention scores\n",
        "        scale = 1.0 / math.sqrt(self.dim_head)\n",
        "        attn = torch.matmul(q, k.transpose(-2, -1)) * scale\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "\n",
        "        # Apply attention to values\n",
        "        out = torch.matmul(attn, v)\n",
        "        out = out.transpose(1, 2).contiguous().view(out.shape[0], -1, self.heads * self.dim_head)\n",
        "        out = self.to_out(out)\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        # First residual connection\n",
        "        x = x + out\n",
        "\n",
        "        # Layer norm 2\n",
        "        normed_x = self.norm2(x)\n",
        "\n",
        "        # Feed-forward network\n",
        "        ff_out = self.ff(normed_x)\n",
        "\n",
        "        # Second residual connection\n",
        "        return x + ff_out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "style_transfer_model"
      },
      "outputs": [],
      "source": [
        "# Complete StripedHyena Style Transfer Model\n",
        "class StripedHyenaStyleTransfer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 content_layers=['conv4_2'],\n",
        "                 style_layers=['conv1_1', 'conv2_1', 'conv3_1', 'conv4_1', 'conv5_1']):\n",
        "        super().__init__()\n",
        "\n",
        "        self.content_layers = content_layers\n",
        "        self.style_layers = style_layers\n",
        "\n",
        "        # VGG16 for feature extraction (pre-trained)\n",
        "        vgg = torchvision.models.vgg16(pretrained=True).features\n",
        "        self.vgg = nn.ModuleList()\n",
        "        self.vgg_layer_names = []\n",
        "\n",
        "        # Create sequential modules from VGG16\n",
        "        i = 0\n",
        "        for layer in vgg.children():\n",
        "            if isinstance(layer, nn.Conv2d):\n",
        "                i += 1\n",
        "                name = f'conv{i}'\n",
        "            elif isinstance(layer, nn.ReLU):\n",
        "                name = f'relu{i}'\n",
        "                layer = nn.ReLU(inplace=False)  # Use non-inplace ReLU\n",
        "            elif isinstance(layer, nn.MaxPool2d):\n",
        "                name = f'pool{i}'\n",
        "            elif isinstance(layer, nn.BatchNorm2d):\n",
        "                name = f'bn{i}'\n",
        "            else:\n",
        "                raise RuntimeError(f'Unrecognized layer: {layer.__class__.__name__}')\n",
        "\n",
        "            self.vgg.add_module(name, layer)\n",
        "            self.vgg_layer_names.append(name)\n",
        "\n",
        "        # Freeze VGG parameters\n",
        "        for param in self.vgg.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # StripedHyena blocks for content and style processing\n",
        "        self.content_hyena = nn.ModuleList([\n",
        "            StripedHyenaBlock(512, heads=8, dim_head=64, dropout=0.1)\n",
        "            for _ in range(3)\n",
        "        ])\n",
        "\n",
        "        self.style_hyena = nn.ModuleList([\n",
        "            StripedHyenaBlock(512, heads=8, dim_head=64, dropout=0.1)\n",
        "            for _ in range(3)\n",
        "        ])\n",
        "\n",
        "        # Decoder network\n",
        "        self.decoder = nn.Sequential(\n",
        "            GatedConv2d(512, 256, kernel_size=3, padding=1),\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "            GatedConv2d(256, 256, kernel_size=3, padding=1),\n",
        "            GatedConv2d(256, 256, kernel_size=3, padding=1),\n",
        "            GatedConv2d(256, 128, kernel_size=3, padding=1),\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "            GatedConv2d(128, 128, kernel_size=3, padding=1),\n",
        "            GatedConv2d(128, 64, kernel_size=3, padding=1),\n",
        "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
        "            GatedConv2d(64, 64, kernel_size=3, padding=1),\n",
        "            nn.Conv2d(64, 3, kernel_size=3, padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode_features(self, x):\n",
        "        \"\"\"Extract VGG features\"\"\"\n",
        "        features = {}\n",
        "        for name, layer in self.vgg._modules.items():\n",
        "            x = layer(x)\n",
        "            if name in self.content_layers or name in self.style_layers:\n",
        "                features[name] = x\n",
        "        return features\n",
        "\n",
        "    def process_content(self, content_features):\n",
        "        \"\"\"Process content features through StripedHyena blocks\"\"\"\n",
        "        # Use the deepest content feature\n",
        "        x = content_features[self.content_layers[-1]]\n",
        "        batch_size, channels, height, width = x.shape\n",
        "\n",
        "        # Reshape to sequence for StripedHyena processing\n",
        "        x = x.view(batch_size, channels, -1).permute(0, 2, 1)  # [batch, seq_len, channels]\n",
        "\n",
        "        # Apply StripedHyena blocks\n",
        "        for block in self.content_hyena:\n",
        "            x = block(x)\n",
        "\n",
        "        # Reshape back to spatial\n",
        "        x = x.permute(0, 2, 1).view(batch_size, channels, height, width)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def process_style(self, style_features):\n",
        "        \"\"\"Process style features through StripedHyena blocks\"\"\"\n",
        "        # Use the deepest style feature\n",
        "        x = style_features[self.style_layers[-1]]\n",
        "        batch_size, channels, height, width = x.shape\n",
        "\n",
        "        # Reshape to sequence for StripedHyena processing\n",
        "        x = x.view(batch_size, channels, -1).permute(0, 2, 1)  # [batch, seq_len, channels]\n",
        "\n",
        "        # Apply StripedHyena blocks\n",
        "        for block in self.style_hyena:\n",
        "            x = block(x)\n",
        "\n",
        "        # Reshape back to spatial\n",
        "        x = x.permute(0, 2, 1).view(batch_size, channels, height, width)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def forward(self, content_img, style_img, alpha=1.0, return_features=False):\n",
        "        \"\"\"Forward pass for style transfer\"\"\"\n",
        "        # Extract features\n",
        "        content_features = self.encode_features(content_img)\n",
        "        style_features = self.encode_features(style_img)\n",
        "\n",
        "        # Process content and style features\n",
        "        processed_content = self.process_content(content_features)\n",
        "        processed_style = self.process_style(style_features)\n",
        "\n",
        "        # Combine content and style features\n",
        "        combined = processed_content * alpha + processed_style * (1 - alpha)\n",
        "\n",
        "        # Decode to generate stylized image\n",
        "        output_img = self.decoder(combined)\n",
        "\n",
        "        if return_features:\n",
        "            return output_img, content_features, style_features, {\n",
        "                'content': processed_content,\n",
        "                'style': style_features  # Use original style features for loss computation\n",
        "            }\n",
        "\n",
        "        return output_img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "initialize_model"
      },
      "outputs": [],
      "source": [
        "# Initialize the model\n",
        "model = StripedHyenaStyleTransfer().to(device)\n",
        "print(f\"Model initialized on {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "basic_demo"
      },
      "source": [
        "## 4. Basic Style Transfer Demo <a name=\"basic_demo\"></a>\n",
        "\n",
        "Let's create a basic demo to demonstrate the style transfer capabilities of our StripedHyena model. We'll use some sample images and apply the style transfer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "image_utils"
      },
      "outputs": [],
      "source": [
        "# Utility functions for image processing\n",
        "def load_image_from_url(url, max_size=512):\n",
        "    \"\"\"Load an image from a URL\"\"\"\n",
        "    response = requests.get(url)\n",
        "    img = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "\n",
        "    # Resize while maintaining aspect ratio\n",
        "    if max(img.size) > max_size:\n",
        "        ratio = max_size / max(img.size)\n",
        "        new_size = (int(img.size[0] * ratio), int(img.size[1] * ratio))\n",
        "        img = img.resize(new_size, Image.LANCZOS)\n",
        "\n",
        "    return img\n",
        "\n",
        "def load_image_from_upload(uploaded_file, max_size=512):\n",
        "    \"\"\"Load an image from an uploaded file\"\"\"\n",
        "    img = Image.open(uploaded_file).convert('RGB')\n",
        "\n",
        "    # Resize while maintaining aspect ratio\n",
        "    if max(img.size) > max_size:\n",
        "        ratio = max_size / max(img.size)\n",
        "        new_size = (int(img.size[0] * ratio), int(img.size[1] * ratio))\n",
        "        img = img.resize(new_size, Image.LANCZOS)\n",
        "\n",
        "    return img\n",
        "\n",
        "def preprocess_image(img):\n",
        "    \"\"\"Convert PIL image to tensor for model input\"\"\"\n",
        "    transform = torchvision.transforms.Compose([\n",
        "        torchvision.transforms.ToTensor(),\n",
        "        torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    img_tensor = transform(img).unsqueeze(0).to(device)\n",
        "    return img_tensor\n",
        "\n",
        "def deprocess_image(tensor):\n",
        "    \"\"\"Convert tensor to PIL image for display\"\"\"\n",
        "    # Move to CPU if needed\n",
        "    if IS_TPU_AVAILABLE:\n",
        "        tensor = xm.mesh_reduce('tensor_to_cpu', tensor, lambda x: x.cpu())\n",
        "    elif tensor.is_cuda:\n",
        "        tensor = tensor.cpu()\n",
        "\n",
        "    # Denormalize\n",
        "    tensor = tensor.squeeze(0).detach().clone()\n",
        "    tensor = tensor.clamp(0, 1)\n",
        "\n",
        "    # Convert to PIL image\n",
        "    img = torchvision.transforms.ToPILImage()(tensor)\n",
        "    return img\n",
        "\n",
        "def display_images(content_img, style_img, output_img, figsize=(15, 5)):\n",
        "    \"\"\"Display content, style, and output images side by side\"\"\"\n",
        "    fig, axes = plt.subplots(1, 3, figsize=figsize)\n",
        "\n",
        "    axes[0].imshow(content_img)\n",
        "    axes[0].set_title(\"Content Image\")\n",
        "    axes[0].axis(\"off\")\n",
        "\n",
        "    axes[1].imshow(style_img)\n",
        "    axes[1].set_title(\"Style Image\")\n",
        "    axes[1].axis(\"off\")\n",
        "\n",
        "    axes[2].imshow(output_img)\n",
        "    axes[2].set_title(\"Stylized Output\")\n",
        "    axes[2].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sample_images"
      },
      "outputs": [],
      "source": [
        "# Sample images for demonstration\n",
        "content_url = \"https://images.pexels.com/photos/2559941/pexels-photo-2559941.jpeg\"\n",
        "style_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg/1280px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg\"\n",
        "\n",
        "try:\n",
        "    content_img = load_image_from_url(content_url)\n",
        "    style_img = load_image_from_url(style_url)\n",
        "\n",
        "    # Display original images\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "    axes[0].imshow(content_img)\n",
        "    axes[0].set_title(\"Content Image\")\n",
        "    axes[0].axis(\"off\")\n",
        "\n",
        "    axes[1].imshow(style_img)\n",
        "    axes[1].set_title(\"Style Image\")\n",
        "    axes[1].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "except Exception as e:\n",
        "    print(f\"Error loading sample images: {e}\")\n",
        "    print(\"Please upload your own images in the next cell.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "basic_style_transfer"
      },
      "outputs": [],
      "source": [
        "# Basic style transfer\n",
        "def perform_style_transfer(content_img, style_img, alpha=1.0):\n",
        "    \"\"\"Perform style transfer using the StripedHyena model\"\"\"\n",
        "    # Preprocess images\n",
        "    content_tensor = preprocess_image(content_img)\n",
        "    style_tensor = preprocess_image(style_img)\n",
        "\n",
        "    # Perform style transfer\n",
        "    with torch.no_grad():\n",
        "        start_time = time.time()\n",
        "        output_tensor = model(content_tensor, style_tensor, alpha=alpha)\n",
        "\n",
        "        # Handle TPU synchronization if needed\n",
        "        if IS_TPU_AVAILABLE:\n",
        "            xm.mark_step()\n",
        "\n",
        "        end_time = time.time()\n",
        "\n",
        "    # Deprocess output\n",
        "    output_img = deprocess_image(output_tensor)\n",
        "\n",
        "    print(f\"Style transfer completed in {end_time - start_time:.2f} seconds\")\n",
        "    return output_img\n",
        "\n",
        "# Run style transfer on sample images\n",
        "try:\n",
        "    output_img = perform_style_transfer(content_img, style_img, alpha=0.8)\n",
        "    display_images(content_img, style_img, output_img)\n",
        "except NameError:\n",
        "    print(\"Please upload content and style images first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "advanced_demo"
      },
      "source": [
        "## 5. Advanced Style Transfer with Parameter Tuning <a name=\"advanced_demo\"></a>\n",
        "\n",
        "Now, let's create a more advanced demo that allows for parameter tuning and interactive experimentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upload_images"
      },
      "outputs": [],
      "source": [
        "# Upload your own images\n",
        "from google.colab import files\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "def upload_images():\n",
        "    print(\"Please upload a content image:\")\n",
        "    content_file = files.upload()\n",
        "    content_filename = list(content_file.keys())[0]\n",
        "    content_img = load_image_from_upload(content_filename)\n",
        "\n",
        "    print(\"\\nPlease upload a style image:\")\n",
        "    style_file = files.upload()\n",
        "    style_filename = list(style_file.keys())[0]\n",
        "    style_img = load_image_from_upload(style_filename)\n",
        "\n",
        "    return content_img, style_img\n",
        "\n",
        "# Uncomment to upload your own images\n",
        "# custom_content_img, custom_style_img = upload_images()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "interactive_demo"
      },
      "outputs": [],
      "source": [
        "# Interactive demo with parameter tuning\n",
        "def interactive_style_transfer(content_img, style_img):\n",
        "    # Create widgets\n",
        "    alpha_slider = widgets.FloatSlider(\n",
        "        value=0.8,\n",
        "        min=0.0,\n",
        "        max=1.0,\n",
        "        step=0.05,\n",
        "        description='Content Weight:',\n",
        "        continuous_update=False\n",
        "    )\n",
        "\n",
        "    run_button = widgets.Button(description=\"Apply Style Transfer\")\n",
        "    output = widgets.Output()\n",
        "\n",
        "    # Display original images\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "    axes[0].imshow(content_img)\n",
        "    axes[0].set_title(\"Content Image\")\n",
        "    axes[0].axis(\"off\")\n",
        "\n",
        "    axes[1].imshow(style_img)\n",
        "    axes[1].set_title(\"Style Image\")\n",
        "    axes[1].axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Define button click handler\n",
        "    def on_button_clicked(b):\n",
        "        with output:\n",
        "            clear_output()\n",
        "            print(f\"Running style transfer with content weight = {alpha_slider.value}...\")\n",
        "            output_img = perform_style_transfer(content_img, style_img, alpha=alpha_slider.value)\n",
        "            display_images(content_img, style_img, output_img)\n",
        "\n",
        "    # Connect button to handler\n",
        "    run_button.on_click(on_button_clicked)\n",
        "\n",
        "    # Display widgets\n",
        "    display(alpha_slider, run_button, output)\n",
        "\n",
        "# Run interactive demo with sample images\n",
        "try:\n",
        "    interactive_style_transfer(content_img, style_img)\n",
        "except NameError:\n",
        "    print(\"Please upload content and style images first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "custom_training"
      },
      "source": [
        "## 6. Training on Custom Datasets <a name=\"custom_training\"></a>\n",
        "\n",
        "In this section, we'll implement the training process for the StripedHyena neural style transfer model using custom datasets. This allows you to train the model on your own collection of content and style images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dataset_loader"
      },
      "outputs": [],
      "source": [
        "# Dataset and DataLoader implementation\n",
        "import os\n",
        "import random\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class StyleTransferDataset(Dataset):\n",
        "    \"\"\"Dataset for neural style transfer training\"\"\"\n",
        "\n",
        "    def __init__(self, content_dir, style_dir, image_size=256, transform=None):\n",
        "        \"\"\"\n",
        "        Initialize the dataset.\n",
        "\n",
        "        Args:\n",
        "            content_dir: Directory containing content images\n",
        "            style_dir: Directory containing style images\n",
        "            image_size: Size to resize images to\n",
        "            transform: Optional transform to apply to images\n",
        "        \"\"\"\n",
        "        self.content_paths = []\n",
        "        self.style_paths = []\n",
        "\n",
        "        # Check if directories exist\n",
        "        if os.path.exists(content_dir):\n",
        "            self.content_paths = [os.path.join(content_dir, f) for f in os.listdir(content_dir)\n",
        "                                if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "        else:\n",
        "            print(f\"Warning: Content directory {content_dir} not found.\")\n",
        "\n",
        "        if os.path.exists(style_dir):\n",
        "            self.style_paths = [os.path.join(style_dir, f) for f in os.listdir(style_dir)\n",
        "                               if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
        "        else:\n",
        "            print(f\"Warning: Style directory {style_dir} not found.\")\n",
        "\n",
        "        if not self.content_paths:\n",
        "            raise ValueError(f\"No content images found in {content_dir}\")\n",
        "\n",
        "        if not self.style_paths:\n",
        "            raise ValueError(f\"No style images found in {style_dir}\")\n",
        "\n",
        "        self.image_size = image_size\n",
        "\n",
        "        if transform is None:\n",
        "            self.transform = torchvision.transforms.Compose([\n",
        "                torchvision.transforms.Resize(image_size),\n",
        "                torchvision.transforms.CenterCrop(image_size),\n",
        "                torchvision.transforms.ToTensor(),\n",
        "                torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                              std=[0.229, 0.224, 0.225])\n",
        "            ])\n",
        "        else:\n",
        "            self.transform = transform\n",
        "\n",
        "        print(f\"Found {len(self.content_paths)} content images and {len(self.style_paths)} style images\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.content_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Load content image\n",
        "        content_path = self.content_paths[idx]\n",
        "        content_img = Image.open(content_path).convert('RGB')\n",
        "        content_tensor = self.transform(content_img)\n",
        "\n",
        "        # Randomly select a style image\n",
        "        style_path = random.choice(self.style_paths)\n",
        "        style_img = Image.open(style_path).convert('RGB')\n",
        "        style_tensor = self.transform(style_img)\n",
        "\n",
        "        return {\n",
        "            'content': content_tensor,\n",
        "            'style': style_tensor,\n",
        "            'content_path': content_path,\n",
        "            'style_path': style_path\n",
        "        }\n",
        "\n",
        "def get_dataloader(content_dir, style_dir, batch_size=4, image_size=256, num_workers=2):\n",
        "    \"\"\"Create and return the data loader\"\"\"\n",
        "    dataset = StyleTransferDataset(content_dir, style_dir, image_size)\n",
        "\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=True if torch.cuda.is_available() else False\n",
        "    )\n",
        "\n",
        "    return dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "loss_functions"
      },
      "outputs": [],
      "source": [
        "# Loss functions for neural style transfer\n",
        "class ContentLoss(nn.Module):\n",
        "    \"\"\"Content loss for neural style transfer\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(ContentLoss, self).__init__()\n",
        "        self.criterion = nn.MSELoss()\n",
        "\n",
        "    def forward(self, output_features, target_features):\n",
        "        return self.criterion(output_features, target_features)\n",
        "\n",
        "class StyleLoss(nn.Module):\n",
        "    \"\"\"Style loss for neural style transfer\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(StyleLoss, self).__init__()\n",
        "        self.criterion = nn.MSELoss()\n",
        "\n",
        "    def forward(self, output_features, target_features):\n",
        "        # Calculate Gram matrices\n",
        "        output_gram = self.gram_matrix(output_features)\n",
        "        target_gram = self.gram_matrix(target_features)\n",
        "        return self.criterion(output_gram, target_gram)\n",
        "\n",
        "    def gram_matrix(self, features):\n",
        "        \"\"\"Calculate Gram matrix for style loss\"\"\"\n",
        "        batch_size, ch, h, w = features.size()\n",
        "        features = features.view(batch_size, ch, h * w)\n",
        "        features_t = features.transpose(1, 2)\n",
        "        gram = features.bmm(features_t) / (ch * h * w)\n",
        "        return gram\n",
        "\n",
        "class TVLoss(nn.Module):\n",
        "    \"\"\"Total variation loss for smoothing\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super(TVLoss, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, c, h, w = x.size()\n",
        "        tv_h = torch.pow(x[:, :, 1:, :] - x[:, :, :-1, :], 2).sum()\n",
        "        tv_w = torch.pow(x[:, :, :, 1:] - x[:, :, :, :-1], 2).sum()\n",
        "        return (tv_h + tv_w) / (batch_size * c * h * w)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "training_function"
      },
      "outputs": [],
      "source": [
        "# Training function\n",
        "def train_model(content_dir, style_dir,\n",
        "                num_epochs=100,\n",
        "                batch_size=4,\n",
        "                learning_rate=1e-4,\n",
        "                content_weight=1.0,\n",
        "                style_weight=10.0,\n",
        "                tv_weight=0.001,\n",
        "                checkpoint_interval=10,\n",
        "                image_size=256):\n",
        "    \"\"\"Train the StripedHyena neural style transfer model\"\"\"\n",
        "\n",
        "    # Create output directories\n",
        "    os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "    os.makedirs(\"results\", exist_ok=True)\n",
        "\n",
        "    # Initialize model\n",
        "    model = StripedHyenaStyleTransfer().to(device)\n",
        "\n",
        "    # Initialize loss functions\n",
        "    content_criterion = ContentLoss().to(device)\n",
        "    style_criterion = StyleLoss().to(device)\n",
        "    tv_criterion = TVLoss().to(device)\n",
        "\n",
        "    # Initialize optimizer\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    # Get data loader\n",
        "    dataloader = get_dataloader(content_dir, style_dir, batch_size, image_size)\n",
        "\n",
        "    # Training loop\n",
        "    losses = []\n",
        "\n",
        "    print(f\"Starting training for {num_epochs} epochs...\")\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0.0\n",
        "        start_time = time.time()\n",
        "\n",
        "        model.train()\n",
        "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        for batch in progress_bar:\n",
        "            # Move data to device\n",
        "            content_images = batch['content'].to(device)\n",
        "            style_images = batch['style'].to(device)\n",
        "\n",
        "            # Zero gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            output_images, content_features, style_features, output_features = model(\n",
        "                content_images, style_images, return_features=True\n",
        "            )\n",
        "\n",
        "            # Calculate losses\n",
        "            content_loss = content_criterion(output_features['content'], content_features[model.content_layers[0]])\n",
        "\n",
        "            style_loss = 0\n",
        "            for layer in model.style_layers:\n",
        "                style_loss += style_criterion(output_features['style'][layer], style_features[layer])\n",
        "            style_loss /= len(model.style_layers)\n",
        "\n",
        "            tv_loss = tv_criterion(output_images)\n",
        "\n",
        "            # Total loss\n",
        "            loss = content_weight * content_loss + style_weight * style_loss + tv_weight * tv_loss\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Handle TPU synchronization if needed\n",
        "            if IS_TPU_AVAILABLE:\n",
        "                xm.mark_step()\n",
        "\n",
        "            # Update progress\n",
        "            epoch_loss += loss.item()\n",
        "            progress_bar.set_postfix({\n",
        "                \"loss\": f\"{loss.item():.4f}\",\n",
        "                \"content\": f\"{content_loss.item():.4f}\",\n",
        "                \"style\": f\"{style_loss.item():.4f}\"\n",
        "            })\n",
        "\n",
        "        # Calculate average epoch loss\n",
        "        avg_epoch_loss = epoch_loss / len(dataloader)\n",
        "        losses.append(avg_epoch_loss)\n",
        "\n",
        "        # Print epoch summary\n",
        "        elapsed_time = time.time() - start_time\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_epoch_loss:.4f} - Time: {elapsed_time:.2f}s\")\n",
        "\n",
        "        # Save checkpoint\n",
        "        if (epoch + 1) % checkpoint_interval == 0:\n",
        "            checkpoint_path = f\"checkpoints/model_epoch_{epoch+1}.pth\"\n",
        "            torch.save({\n",
        "                'epoch': epoch + 1,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': avg_epoch_loss,\n",
        "            }, checkpoint_path)\n",
        "            print(f\"Checkpoint saved to {checkpoint_path}\")\n",
        "\n",
        "    # Save final model\n",
        "    torch.save(model.state_dict(), \"checkpoints/model_final.pth\")\n",
        "    print(\"Training complete. Final model saved.\")\n",
        "\n",
        "    # Plot loss curve\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.plot(range(1, num_epochs + 1), losses)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training Loss')\n",
        "    plt.grid(True)\n",
        "    plt.savefig('results/training_loss.png')\n",
        "    plt.show()\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prepare_dataset"
      },
      "outputs": [],
      "source": [
        "# Prepare dataset directories and download sample images\n",
        "def prepare_sample_dataset(num_images=5):\n",
        "    \"\"\"Prepare a sample dataset for training\"\"\"\n",
        "    # Create directories\n",
        "    os.makedirs(\"dataset/content\", exist_ok=True)\n",
        "    os.makedirs(\"dataset/style\", exist_ok=True)\n",
        "\n",
        "    # Sample content image URLs\n",
        "    content_urls = [\n",
        "        \"https://images.pexels.com/photos/2559941/pexels-photo-2559941.jpeg\",\n",
        "        \"https://images.pexels.com/photos/1563256/pexels-photo-1563256.jpeg\",\n",
        "        \"https://images.pexels.com/photos/1366630/pexels-photo-1366630.jpeg\",\n",
        "        \"https://images.pexels.com/photos/1366909/pexels-photo-1366909.jpeg\",\n",
        "        \"https://images.pexels.com/photos/1366919/pexels-photo-1366919.jpeg\"\n",
        "    ]\n",
        "\n",
        "    # Sample style image URLs\n",
        "    style_urls = [\n",
        "        \"https://upload.wikimedia.org/wikipedia/commons/thumb/e/ea/Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg/1280px-Van_Gogh_-_Starry_Night_-_Google_Art_Project.jpg\",\n",
        "        \"https://upload.wikimedia.org/wikipedia/commons/thumb/c/cc/Grant_Wood_-_American_Gothic_-_Google_Art_Project.jpg/1280px-Grant_Wood_-_American_Gothic_-_Google_Art_Project.jpg\",\n",
        "        \"https://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Picasso_Three_Musicians_MoMA.jpg/1280px-Picasso_Three_Musicians_MoMA.jpg\",\n",
        "        \"https://upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Vassily_Kandinsky%2C_1913_-_Composition_7.jpg/1280px-Vassily_Kandinsky%2C_1913_-_Composition_7.jpg\",\n",
        "        \"https://upload.wikimedia.org/wikipedia/commons/thumb/a/a5/Tsunami_by_hokusai_19th_century.jpg/1280px-Tsunami_by_hokusai_19th_century.jpg\"\n",
        "    ]\n",
        "\n",
        "    # Download content images\n",
        "    print(\"Downloading content images...\")\n",
        "    for i, url in enumerate(content_urls[:num_images]):\n",
        "        try:\n",
        "            response = requests.get(url, timeout=10)\n",
        "            if response.status_code == 200:\n",
        "                with open(f\"dataset/content/content_{i+1}.jpg\", \"wb\") as f:\n",
        "                    f.write(response.content)\n",
        "                print(f\"Downloaded content image {i+1}/{num_images}\")\n",
        "            else:\n",
        "                print(f\"Failed to download content image {i+1}: HTTP {response.status_code}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error downloading content image {i+1}: {e}\")\n",
        "\n",
        "    # Download style images\n",
        "    print(\"\\nDownloading style images...\")\n",
        "    for i, url in enumerate(style_urls[:num_images]):\n",
        "        try:\n",
        "            response = requests.get(url, timeout=10)\n",
        "            if response.status_code == 200:\n",
        "                with open(f\"dataset/style/style_{i+1}.jpg\", \"wb\") as f:\n",
        "                    f.write(response.content)\n",
        "                print(f\"Downloaded style image {i+1}/{num_images}\")\n",
        "            else:\n",
        "                print(f\"Failed to download style image {i+1}: HTTP {response.status_code}\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error downloading style image {i+1}: {e}\")\n",
        "\n",
        "    print(\"\\nSample dataset prepared.\")\n",
        "    return \"dataset/content\", \"dataset/style\"\n",
        "\n",
        "# Uncomment to prepare a sample dataset\n",
        "# content_dir, style_dir = prepare_sample_dataset(num_images=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upload_dataset"
      },
      "outputs": [],
      "source": [
        "# Upload your own dataset\n",
        "def upload_dataset():\n",
        "    \"\"\"Upload custom dataset for training\"\"\"\n",
        "    # Create directories\n",
        "    os.makedirs(\"dataset/content\", exist_ok=True)\n",
        "    os.makedirs(\"dataset/style\", exist_ok=True)\n",
        "\n",
        "    print(\"Please upload content images (you can select multiple files):\")\n",
        "    content_files = files.upload()\n",
        "\n",
        "    # Save content images\n",
        "    for filename, content in content_files.items():\n",
        "        with open(f\"dataset/content/{filename}\", \"wb\") as f:\n",
        "            f.write(content)\n",
        "\n",
        "    print(f\"\\nUploaded {len(content_files)} content images.\")\n",
        "\n",
        "    print(\"\\nPlease upload style images (you can select multiple files):\")\n",
        "    style_files = files.upload()\n",
        "\n",
        "    # Save style images\n",
        "    for filename, content in style_files.items():\n",
        "        with open(f\"dataset/style/{filename}\", \"wb\") as f:\n",
        "            f.write(content)\n",
        "\n",
        "    print(f\"\\nUploaded {len(style_files)} style images.\")\n",
        "\n",
        "    return \"dataset/content\", \"dataset/style\"\n",
        "\n",
        "# Uncomment to upload your own dataset\n",
        "# content_dir, style_dir = upload_dataset()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "run_training"
      },
      "outputs": [],
      "source": [
        "# Run training with interactive parameters\n",
        "def interactive_training():\n",
        "    \"\"\"Interactive training with parameter selection\"\"\"\n",
        "    # Create widgets for dataset selection\n",
        "    dataset_options = widgets.RadioButtons(\n",
        "        options=['Use sample dataset', 'Upload my own dataset'],\n",
        "        description='Dataset:',\n",
        "        disabled=False\n",
        "    )\n",
        "\n",
        "    # Create widgets for training parameters\n",
        "    num_epochs_slider = widgets.IntSlider(\n",
        "        value=20,\n",
        "        min=5,\n",
        "        max=100,\n",
        "        step=5,\n",
        "        description='Epochs:',\n",
        "        disabled=False\n",
        "    )\n",
        "\n",
        "    batch_size_slider = widgets.IntSlider(\n",
        "        value=2,\n",
        "        min=1,\n",
        "        max=8,\n",
        "        step=1,\n",
        "        description='Batch Size:',\n",
        "        disabled=False\n",
        "    )\n",
        "\n",
        "    content_weight_slider = widgets.FloatSlider(\n",
        "        value=1.0,\n",
        "        min=0.1,\n",
        "        max=10.0,\n",
        "        step=0.1,\n",
        "        description='Content Weight:',\n",
        "        disabled=False\n",
        "    )\n",
        "\n",
        "    style_weight_slider = widgets.FloatSlider(\n",
        "        value=10.0,\n",
        "        min=1.0,\n",
        "        max=50.0,\n",
        "        step=1.0,\n",
        "        description='Style Weight:',\n",
        "        disabled=False\n",
        "    )\n",
        "\n",
        "    start_button = widgets.Button(\n",
        "        description='Start Training',\n",
        "        disabled=False,\n",
        "        button_style='success',\n",
        "        tooltip='Click to start training'\n",
        "    )\n",
        "\n",
        "    output = widgets.Output()\n",
        "\n",
        "    # Display widgets\n",
        "    display(dataset_options)\n",
        "    display(widgets.HBox([num_epochs_slider, batch_size_slider]))\n",
        "    display(widgets.HBox([content_weight_slider, style_weight_slider]))\n",
        "    display(start_button)\n",
        "    display(output)\n",
        "\n",
        "    # Define button click handler\n",
        "    def on_button_clicked(b):\n",
        "        with output:\n",
        "            clear_output()\n",
        "\n",
        "            # Get dataset\n",
        "            if dataset_options.value == 'Use sample dataset':\n",
        "                content_dir, style_dir = prepare_sample_dataset(num_images=3)\n",
        "            else:\n",
        "                content_dir, style_dir = upload_dataset()\n",
        "\n",
        "            # Get training parameters\n",
        "            num_epochs = num_epochs_slider.value\n",
        "            batch_size = batch_size_slider.value\n",
        "            content_weight = content_weight_slider.value\n",
        "            style_weight = style_weight_slider.value\n",
        "\n",
        "            print(f\"Starting training with the following parameters:\")\n",
        "            print(f\"- Number of epochs: {num_epochs}\")\n",
        "            print(f\"- Batch size: {batch_size}\")\n",
        "            print(f\"- Content weight: {content_weight}\")\n",
        "            print(f\"- Style weight: {style_weight}\")\n",
        "            print(f\"- Content directory: {content_dir}\")\n",
        "            print(f\"- Style directory: {style_dir}\")\n",
        "            print(\"\\n\")\n",
        "\n",
        "            # Run training\n",
        "            trained_model = train_model(\n",
        "                content_dir=content_dir,\n",
        "                style_dir=style_dir,\n",
        "                num_epochs=num_epochs,\n",
        "                batch_size=batch_size,\n",
        "                content_weight=content_weight,\n",
        "                style_weight=style_weight,\n",
        "                checkpoint_interval=5,\n",
        "                image_size=256\n",
        "            )\n",
        "\n",
        "            print(\"\\nTraining complete! You can now use the trained model for style transfer.\")\n",
        "\n",
        "    # Connect button to handler\n",
        "    start_button.on_click(on_button_clicked)\n",
        "\n",
        "# Uncomment to run interactive training\n",
        "# interactive_training()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "performance"
      },
      "source": [
        "## 7. Performance Analysis: TPU vs. CPU vs. GPU <a name=\"performance\"></a>\n",
        "\n",
        "In this section, we'll compare the performance of the StripedHyena neural style transfer model on different hardware accelerators: TPU, GPU, and CPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "performance_comparison"
      },
      "outputs": [],
      "source": [
        "# Performance comparison function\n",
        "def compare_performance(content_img, style_img, image_sizes=[256, 512, 1024]):\n",
        "    \"\"\"Compare performance of style transfer on different devices and image sizes\"\"\"\n",
        "    results = []\n",
        "\n",
        "    # Check available devices\n",
        "    available_devices = []\n",
        "    device_names = []\n",
        "\n",
        "    if IS_TPU_AVAILABLE:\n",
        "        available_devices.append(xm.xla_device())\n",
        "        device_names.append(\"TPU\")\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        available_devices.append(torch.device(\"cuda\"))\n",
        "        device_names.append(f\"GPU ({torch.cuda.get_device_name(0)})\")\n",
        "\n",
        "    available_devices.append(torch.device(\"cpu\"))\n",
        "    device_names.append(\"CPU\")\n",
        "\n",
        "    print(f\"Testing performance on {len(available_devices)} devices: {', '.join(device_names)}\")\n",
        "\n",
        "    # Test each device and image size\n",
        "    for size in image_sizes:\n",
        "        print(f\"\\nTesting with image size: {size}x{size}\")\n",
        "\n",
        "        # Resize images\n",
        "        content_resized = content_img.resize((size, size), Image.LANCZOS)\n",
        "        style_resized = style_img.resize((size, size), Image.LANCZOS)\n",
        "\n",
        "        for device, device_name in zip(available_devices, device_names):\n",
        "            print(f\"  Testing on {device_name}...\")\n",
        "\n",
        "            # Initialize model on device\n",
        "            model = StripedHyenaStyleTransfer().to(device)\n",
        "            model.eval()\n",
        "\n",
        "            # Preprocess images\n",
        "            transform = torchvision.transforms.Compose([\n",
        "                torchvision.transforms.ToTensor(),\n",
        "                torchvision.transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "            ])\n",
        "\n",
        "            content_tensor = transform(content_resized).unsqueeze(0).to(device)\n",
        "            style_tensor = transform(style_resized).unsqueeze(0).to(device)\n",
        "\n",
        "            # Warm-up run\n",
        "            with torch.no_grad():\n",
        "                _ = model(content_tensor, style_tensor)\n",
        "                if device_name == \"TPU\":\n",
        "                    xm.mark_step()\n",
        "\n",
        "            # Timed run\n",
        "            torch.cuda.synchronize() if device_name.startswith(\"GPU\") else None\n",
        "            start_time = time.time()\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for _ in range(3):  # Run multiple times for more accurate timing\n",
        "                    _ = model(content_tensor, style_tensor)\n",
        "                    if device_name == \"TPU\":\n",
        "                        xm.mark_step()\n",
        "\n",
        "            torch.cuda.synchronize() if device_name.startswith(\"GPU\") else None\n",
        "            end_time = time.time()\n",
        "\n",
        "            # Calculate average time\n",
        "            avg_time = (end_time - start_time) / 3\n",
        "            print(f\"    Average time: {avg_time:.4f} seconds\")\n",
        "\n",
        "            results.append({\n",
        "                \"device\": device_name,\n",
        "                \"image_size\": size,\n",
        "                \"time\": avg_time\n",
        "            })\n",
        "\n",
        "    # Plot results\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    for device_name in device_names:\n",
        "        device_results = [r for r in results if r[\"device\"] == device_name]\n",
        "        sizes = [r[\"image_size\"] for r in device_results]\n",
        "        times = [r[\"time\"] for r in device_results]\n",
        "        plt.plot(sizes, times, marker='o', label=device_name)\n",
        "\n",
        "    plt.xlabel('Image Size (pixels)')\n",
        "    plt.ylabel('Processing Time (seconds)')\n",
        "    plt.title('StripedHyena Neural Style Transfer Performance Comparison')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.savefig('results/performance_comparison.png')\n",
        "    plt.show()\n",
        "\n",
        "    return results\n",
        "\n",
        "# Uncomment to run performance comparison\n",
        "# try:\n",
        "#     performance_results = compare_performance(content_img, style_img, image_sizes=[256, 512])\n",
        "# except NameError:\n",
        "#     print(\"Please load content and style images first.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "export"
      },
      "source": [
        "## 8. Exporting Your Model <a name=\"export\"></a>\n",
        "\n",
        "In this section, we'll show how to export your trained model for deployment or sharing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "export_model"
      },
      "outputs": [],
      "source": [
        "# Export model function\n",
        "def export_model(model_path=\"checkpoints/model_final.pth\"):\n",
        "    \"\"\"Export the trained model for deployment\"\"\"\n",
        "    # Create export directory\n",
        "    os.makedirs(\"export\", exist_ok=True)\n",
        "\n",
        "    # Check if model exists\n",
        "    if not os.path.exists(model_path):\n",
        "        print(f\"Model file {model_path} not found. Please train a model first.\")\n",
        "        return\n",
        "\n",
        "    # Load model\n",
        "    model = StripedHyenaStyleTransfer().to(device)\n",
        "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
        "    model.eval()\n",
        "\n",
        "    # Export to TorchScript\n",
        "    print(\"Exporting model to TorchScript...\")\n",
        "    example_content = torch.randn(1, 3, 256, 256).to(device)\n",
        "    example_style = torch.randn(1, 3, 256, 256).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        traced_model = torch.jit.trace(model, (example_content, example_style))\n",
        "        traced_model.save(\"export/model_traced.pt\")\n",
        "\n",
        "    print(\"Model exported to export/model_traced.pt\")\n",
        "\n",
        "    # Create a simple inference script\n",
        "    inference_script = \"\"\"\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "\n",
        "def load_image(image_path, max_size=512):\n",
        "    \"\"\"Load and preprocess an image\"\"\"\n",
        "    img = Image.open(image_path).convert('RGB')\n",
        "\n",
        "    # Resize while maintaining aspect ratio\n",
        "    if max(img.size) > max_size:\n",
        "        ratio = max_size / max(img.size)\n",
        "        new_size = (int(img.size[0] * ratio), int(img.size[1] * ratio))\n",
        "        img = img.resize(new_size, Image.LANCZOS)\n",
        "\n",
        "    return img\n",
        "\n",
        "def preprocess_image(img):\n",
        "    \"\"\"Convert PIL image to tensor for model input\"\"\"\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "    img_tensor = transform(img).unsqueeze(0)\n",
        "    return img_tensor\n",
        "\n",
        "def deprocess_image(tensor):\n",
        "    \"\"\"Convert tensor to PIL image for display\"\"\"\n",
        "    tensor = tensor.squeeze(0).detach().clone()\n",
        "    tensor = tensor.clamp(0, 1)\n",
        "\n",
        "    # Convert to PIL image\n",
        "    img = transforms.ToPILImage()(tensor)\n",
        "    return img\n",
        "\n",
        "def apply_style_transfer(model_path, content_path, style_path, output_path, alpha=0.8):\n",
        "    \"\"\"Apply style transfer using the exported model\"\"\"\n",
        "    # Load model\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = torch.jit.load(model_path).to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Load images\n",
        "    content_img = load_image(content_path)\n",
        "    style_img = load_image(style_path)\n",
        "\n",
        "    # Preprocess images\n",
        "    content_tensor = preprocess_image(content_img).to(device)\n",
        "    style_tensor = preprocess_image(style_img).to(device)\n",
        "\n",
        "    # Apply style transfer\n",
        "    with torch.no_grad():\n",
        "        output_tensor = model(content_tensor, style_tensor)\n",
        "\n",
        "    # Save result\n",
        "    output_img = deprocess_image(output_tensor)\n",
        "    output_img.save(output_path)\n",
        "    print(f\"Stylized image saved to {output_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser(description=\"Apply neural style transfer using StripedHyena model\")\n",
        "    parser.add_argument(\"--model\", type=str, default=\"model_traced.pt\", help=\"Path to the exported model\")\n",
        "    parser.add_argument(\"--content\", type=str, required=True, help=\"Path to the content image\")\n",
        "    parser.add_argument(\"--style\", type=str, required=True, help=\"Path to the style image\")\n",
        "    parser.add_argument(\"--output\", type=str, default=\"stylized_output.jpg\", help=\"Path to save the output image\")\n",
        "    parser.add_argument(\"--alpha\", type=float, default=0.8, help=\"Content weight (0.0 to 1.0)\")\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    apply_style_transfer(args.model, args.content, args.style, args.output, args.alpha)\n",
        "\"\"\"\n",
        "\n",
        "    with open(\"export/inference.py\", \"w\") as f:\n",
        "        f.write(inference_script)\n",
        "\n",
        "    print(\"Inference script saved to export/inference.py\")\n",
        "\n",
        "    # Create a README file\n",
        "    readme = \"\"\"\n",
        "# StripedHyena Neural Style Transfer\n",
        "\n",
        "This package contains a trained StripedHyena neural style transfer model.\n",
        "\n",
        "## Contents\n",
        "\n",
        "- `model_traced.pt`: The exported TorchScript model\n",
        "- `inference.py`: A script for applying style transfer\n",
        "\n",
        "## Requirements\n",
        "\n",
        "- Python 3.8+\n",
        "- PyTorch 2.0+\n",
        "- torchvision\n",
        "- Pillow\n",
        "\n",
        "## Usage\n",
        "\n",
        "```bash\n",
        "python inference.py --content path/to/content.jpg --style path/to/style.jpg --output stylized_output.jpg\n",
        "```\n",
        "\n",
        "## Parameters\n",
        "\n",
        "- `--model`: Path to the exported model (default: model_traced.pt)\n",
        "- `--content`: Path to the content image\n",
        "- `--style`: Path to the style image\n",
        "- `--output`: Path to save the output image (default: stylized_output.jpg)\n",
        "- `--alpha`: Content weight (0.0 to 1.0, default: 0.8)\n",
        "\"\"\"\n",
        "\n",
        "    with open(\"export/README.md\", \"w\") as f:\n",
        "        f.write(readme)\n",
        "\n",
        "    print(\"README file saved to export/README.md\")\n",
        "\n",
        "    # Create a zip file\n",
        "    !zip -r export/striped_hyena_style_transfer.zip export/model_traced.pt export/inference.py export/README.md\n",
        "\n",
        "    print(\"\\nModel exported successfully! You can download the zip file from the Files tab.\")\n",
        "    return \"export/striped_hyena_style_transfer.zip\"\n",
        "\n",
        "# Uncomment to export a trained model\n",
        "# export_path = export_model()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "download_export"
      },
      "outputs": [],
      "source": [
        "# Download the exported model\n",
        "def download_export(export_path=\"export/striped_hyena_style_transfer.zip\"):\n",
        "    \"\"\"Download the exported model\"\"\"\n",
        "    from google.colab import files\n",
        "\n",
        "    if os.path.exists(export_path):\n",
        "        files.download(export_path)\n",
        "        print(f\"Downloaded {export_path}\")\n",
        "    else:\n",
        "        print(f\"Export file {export_path} not found. Please export the model first.\")\n",
        "\n",
        "# Uncomment to download the exported model\n",
        "# download_export()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "conclusion"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "In this notebook, we've implemented and explored the StripedHyena neural style transfer model, optimized for Google Colab's TPU environment. We've covered:\n",
        "\n",
        "1. The architecture and components of the StripedHyena model\n",
        "2. Implementation of the model for neural style transfer\n",
        "3. Basic and advanced style transfer demos\n",
        "4. Training on custom datasets\n",
        "5. Performance comparison across different hardware accelerators\n",
        "6. Exporting the model for deployment\n",
        "\n",
        "The StripedHyena architecture, with its hybrid approach combining rotary attention and gated convolutions, offers an efficient and effective solution for neural style transfer tasks. Its memory efficiency and linear scaling properties make it particularly well-suited for processing high-resolution images.\n",
        "\n",
        "Feel free to experiment with different content and style images, adjust the parameters, and train the model on your own datasets to achieve unique and personalized style transfer results."
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}